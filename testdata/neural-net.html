<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.png"/><link href="https://sirupsen.com/atom.xml" rel="alternate" title="Sirupsen" type="application/atom+xml"/><meta name="color-scheme" content="dark light"/><title>Neural Network From Scratch</title><link rel="preload" as="image" imageSrcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=3840&amp;q=75 3840w" imageSizes="(min-width: 36rem) 36rem 100vw"/><meta name="next-head-count" content="7"/><link rel="preload" href="/_next/static/css/2c36f778945f110e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/2c36f778945f110e.css" data-n-g=""/><link rel="preload" href="/_next/static/css/331081cfef643c4b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/331081cfef643c4b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-6219c82040c3ea44.js" defer=""></script><script src="/_next/static/chunks/framework-3b5a00d5d7e8d93b.js" defer=""></script><script src="/_next/static/chunks/main-971fce106f331d5e.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb38259bd75b1b62.js" defer=""></script><script src="/_next/static/chunks/ae51ba48-0f49030f58239c92.js" defer=""></script><script src="/_next/static/chunks/95b64a6e-01bfe4fee4530feb.js" defer=""></script><script src="/_next/static/chunks/d64684d8-61712f81a0d65af7.js" defer=""></script><script src="/_next/static/chunks/128-81baffabab7ee0d2.js" defer=""></script><script src="/_next/static/chunks/21-b990ea6f2170bcb5.js" defer=""></script><script src="/_next/static/chunks/671-9980cf90cac8f9c7.js" defer=""></script><script src="/_next/static/chunks/388-0a66963a4aa3b2b0.js" defer=""></script><script src="/_next/static/chunks/886-682886878f89fef7.js" defer=""></script><script src="/_next/static/chunks/pages/%5B...id%5D-ed5288bf885085fd.js" defer=""></script><script src="/_next/static/JWaI_7syLZf-2cqKst7XY/_buildManifest.js" defer=""></script><script src="/_next/static/JWaI_7syLZf-2cqKst7XY/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="layout_container__fbLkO" id="container"><header class="layout_header__kY0Lt"><div class="layout_headerOne__iBmV5"><div class="layout_name__WkJlQ"><a href="/static">Simon Hørup Eskildsen</a></div><div class="layout_menuLink__G2L2N"><a href="/about">About</a></div><div class="layout_menuLink__G2L2N"><a href="/static">Blog</a></div><div class="layout_menuLink__G2L2N"><a href="/napkin">Napkin Math</a></div><div class="layout_menuLink__G2L2N"><a href="/books">Books</a></div><div class="layout_menuLink__G2L2N"><a href="/subscribe">Subscribe</a></div><div class="layout_menuLink__G2L2N"><a href="/hire">Hire</a></div></div><div class="layout_headerTwo__XhO_5"><div><a href="https://twitter.com/Sirupsen" title="Twitter" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="utils_socialIcon___4Eo7" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M496 109.5a201.8 201.8 0 01-56.55 15.3 97.51 97.51 0 0043.33-53.6 197.74 197.74 0 01-62.56 23.5A99.14 99.14 0 00348.31 64c-54.42 0-98.46 43.4-98.46 96.9a93.21 93.21 0 002.54 22.1 280.7 280.7 0 01-203-101.3A95.69 95.69 0 0036 130.4c0 33.6 17.53 63.3 44 80.7A97.5 97.5 0 0135.22 199v1.2c0 47 34 86.1 79 95a100.76 100.76 0 01-25.94 3.4 94.38 94.38 0 01-18.51-1.8c12.51 38.5 48.92 66.5 92.05 67.3A199.59 199.59 0 0139.5 405.6a203 203 0 01-23.5-1.4A278.68 278.68 0 00166.74 448c181.36 0 280.44-147.7 280.44-275.8 0-4.2-.11-8.4-.31-12.5A198.48 198.48 0 00496 109.5z"></path></svg></a></div><div><a href="https://mastodon.social/@sirupsen" title="Mastodon" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="utils_socialIcon___4Eo7" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M480 173.59c0-104.13-68.26-134.65-68.26-134.65C377.3 23.15 318.2 16.5 256.8 16h-1.51c-61.4.5-120.46 7.15-154.88 22.94 0 0-68.27 30.52-68.27 134.65 0 23.85-.46 52.35.29 82.59C34.91 358 51.11 458.37 145.32 483.29c43.43 11.49 80.73 13.89 110.76 12.24 54.47-3 85-19.42 85-19.42l-1.79-39.5s-38.93 12.27-82.64 10.77c-43.31-1.48-89-4.67-96-57.81a108.44 108.44 0 01-1-14.9 558.91 558.91 0 0096.39 12.85c32.95 1.51 63.84-1.93 95.22-5.67 60.18-7.18 112.58-44.24 119.16-78.09 10.42-53.34 9.58-130.17 9.58-130.17zm-80.54 134.16h-50V185.38c0-25.8-10.86-38.89-32.58-38.89-24 0-36.06 15.53-36.06 46.24v67h-49.66v-67c0-30.71-12-46.24-36.06-46.24-21.72 0-32.58 13.09-32.58 38.89v122.37h-50V181.67q0-38.65 19.75-61.39c13.6-15.15 31.4-22.92 53.51-22.92 25.58 0 44.95 9.82 57.75 29.48L256 147.69l12.45-20.85c12.81-19.66 32.17-29.48 57.75-29.48 22.11 0 39.91 7.77 53.51 22.92q19.79 22.72 19.75 61.39z"></path></svg></a></div><div><a href="mailto:simon@sirupsen.com" title="Email me" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="utils_socialIcon___4Eo7" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M424 80H88a56.06 56.06 0 00-56 56v240a56.06 56.06 0 0056 56h336a56.06 56.06 0 0056-56V136a56.06 56.06 0 00-56-56zm-14.18 92.63l-144 112a16 16 0 01-19.64 0l-144-112a16 16 0 1119.64-25.26L256 251.73l134.18-104.36a16 16 0 0119.64 25.26z"></path></svg></a></div><div><a href="https://github.com/sirupsen" title="Github" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="utils_socialIcon___4Eo7" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 003.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 01-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0025.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 015-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 01112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 015 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 004-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></div><div><a href="https://www.linkedin.com/in/sirupsen" title="Linkedin" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="utils_socialIcon___4Eo7" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M444.17 32H70.28C49.85 32 32 46.7 32 66.89v374.72C32 461.91 49.85 480 70.28 480h373.78c20.54 0 35.94-18.21 35.94-38.39V66.89C480.12 46.7 464.6 32 444.17 32zm-273.3 373.43h-64.18V205.88h64.18zM141 175.54h-.46c-20.54 0-33.84-15.29-33.84-34.43 0-19.49 13.65-34.42 34.65-34.42s33.85 14.82 34.31 34.42c-.01 19.14-13.31 34.43-34.66 34.43zm264.43 229.89h-64.18V296.32c0-26.14-9.34-44-32.56-44-17.74 0-28.24 12-32.91 23.69-1.75 4.2-2.22 9.92-2.22 15.76v113.66h-64.18V205.88h64.18v27.77c9.34-13.3 23.93-32.44 57.88-32.44 42.13 0 74 27.77 74 87.64z"></path></svg></a></div><div style="margin-left:auto"></div></div></header><article class="utils_article__5xLoK"><h1><span data-br=":Rj8m:" data-brr="1" style="display:inline-block;vertical-align:top;text-decoration:inherit">Neural Network From Scratch</span><script>self.__wrap_balancer=(t, e, n)=>{n=n||document.querySelector(`[data-br="${t}"]`);let o=n.parentElement,r= y=>n.style.maxWidth=y+"px";n.style.maxWidth="";let i=o.clientWidth,s=o.clientHeight,c=i/2,u=i,f;if(i){for(; c+1<u;)f=~~((c+u)/2),r(f),o.clientHeight==s?u=f:c=f;r(u*e+i*(1-e))}};self.__wrap_balancer(":Rj8m:",1)</script></h1><div class="utils_lightText__eUzGY" title="2022-01-03"><time dateTime="2022-01-03">Jan 2022</time></div><nav class="toc"><ol class="toc-level toc-level-1"><li class="toc-item toc-item-h2"><a href="/napkin/neural-net#mental-model-for-a-neural-net-building-one-from-scratch">Mental Model for a Neural Net: Building one from scratch</a></li><li class="toc-item toc-item-h2"><a href="/napkin/neural-net#training-our-neural-network">Training our Neural Network</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a href="/napkin/neural-net#updating-the-hidden-layer-with-gradient-descent">Updating the Hidden Layer with Gradient Descent</a></li></ol></li><li class="toc-item toc-item-h2"><a href="/napkin/neural-net#finalizing-our-neural-network-from-scratch">Finalizing our Neural Network from scratch</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a href="/napkin/neural-net#automagically-computing-the-slope-of-a-function-with-autograd">Automagically computing the slope of a function with autograd</a></li></ol></li><li class="toc-item toc-item-h2"><a href="/napkin/neural-net#ok-so-you-just-implemented-the-most-complicated-average-function-ive-ever-seen">OK, so you just implemented the most complicated average function I’ve ever seen…</a><ol class="toc-level toc-level-2"><li class="toc-item toc-item-h3"><a href="/napkin/neural-net#activation-functions">Activation Functions</a></li><li class="toc-item toc-item-h3"><a href="/napkin/neural-net#matrices">Matrices</a></li></ol></li><li class="toc-item toc-item-h2"><a href="/napkin/neural-net#next-steps-to-implement-your-own-neural-net-from-scratch">Next steps to implement your own neural net from scratch</a></li></ol></nav><p>In this edition of Napkin Math, we’ll invoke the spirit of the Napkin Math
series to establish a mental model for how a neural network works by building
one from scratch. In a future issue we will do napkin math on performance, as
establishing the first-principle understanding is plenty of ground to cover for
today!</p>
<p>Neural nets are increasingly dominating the field of machine learning / artificial
intelligence: the most sophisticated models for computer vision (e.g. CLIP),
natural language processing (e.g. GPT-3), translation (e.g. Google Translate),
and more are based on neural nets. When these artificial neural nets reach some
arbitrary threshold of neurons, we call it <em>deep learning</em>.</p>
<p>A visceral example of Deep Learning’s unreasonable effectiveness comes from
<a href="https://www.listennotes.com/podcasts/the-twiml-ai/systems-and-software-for-xolUkM23Gb0/">this interview</a> with Jeff Dean who leads AI at Google. He explains how
500 lines of Tensorflow outperformed the previous ~500,000 lines of code for
Google Translate’s <em>extremely complicated</em> model. Blew my mind. <sup><a href="/napkin/neural-net#user-content-fn-google">1</a></sup></p>
<p>As a software developer with a predominantly web-related skillset of Ruby,
databases, enough distributed systems knowledge to know to not get fancy, a bit
of hard-earned systems knowledge from debugging incidents, but only high school
level math: <em>neural networks mystify me</em>. How do they work? Why are they so
good? Why are they so slow? Why are GPUs/TPUs used to speed them up? Why do the
biggest models have more neurons than humans, yet still perform worse than the
human brain? <sup><a href="/napkin/neural-net#user-content-fn-gpt3">2</a></sup></p>
<p>In true napkin math fashion, the best course of action to answer those questions
is by implementing a simple neural net from scratch.</p>
<h2 id="mental-model-for-a-neural-net-building-one-from-scratch">Mental Model for a Neural Net: Building one from scratch<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p>The hardest part of napkin math isn’t the calculation itself: it’s acquiring the
conceptual understanding of a system to come up with an equation for its
performance. Presenting and testing mental models of common systems is the crux
of value from the napkin math series!</p>
<p>The simplest neural net we can draw might look something like this:</p>
<figure><img alt="lol" sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmental-model.jpg&amp;w=3840&amp;q=75" width="687" height="598" decoding="async" data-nimg="1" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 687 598&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nGM4ceLEAzD4+fMng7i4OC8v76FDhzZs2MCwYcOG+fPnP3v27O3btwDdaRisnebS+wAAAABJRU5ErkJggg==&#x27;/%3E%3C/svg%3E&quot;)"/></figure>
<ul>
<li><strong>Input layer</strong>. This is a representation of the data that we want to feed to
the neural net. For example, the input layer for a 4x4 pixel grayscale image
that looks like this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[1, 1, 1, 0.2]</title><rect fill="#000000" x="0" y="0" width="1" height="1"></rect><rect fill="#000000" x="1" y="0" width="1" height="1"></rect><rect fill="#000000" x="0" y="1" width="1" height="1"></rect><rect fill="#E3E3E3" x="1" y="1" width="1" height="1"></rect></svg> could be <code>[1, 1, 1, 0.2]</code>. Meaning the first 3 pixels are darkest (1.0) and the last pixel is
lighter (0.2).</li>
<li><strong>Hidden Layer</strong>. This is the layer that does a bunch of math on the input
layer to convert it to our prediction. <em>Training</em> a model refers to changing the
math of the hidden layer(s) to more often create an output like the training
data. We will go into more detail with this layer in a moment. The values in the
hidden layer are called <em>weights</em>.</li>
<li><strong>Output Layer</strong>. This layer will contain our final prediction. For example,
if we feed it the rectangle from before <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[1, 1, 1, 0.2]</title><rect fill="#000000" x="0" y="0" width="1" height="1"></rect><rect fill="#000000" x="1" y="0" width="1" height="1"></rect><rect fill="#000000" x="0" y="1" width="1" height="1"></rect><rect fill="#E3E3E3" x="1" y="1" width="1" height="1"></rect></svg> we
might want the output layer to be a single number to represent how “dark” a
rectangle is, e.g.: <code>0.8</code>.</li>
</ul>
<p>For example for the image <code><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0.8, 0.7, 1, 1]</title><rect fill="#4C4C4C" x="0" y="0" width="1" height="1"></rect><rect fill="#656565" x="1" y="0" width="1" height="1"></rect><rect fill="#000000" x="0" y="1" width="1" height="1"></rect><rect fill="#000000" x="1" y="1" width="1" height="1"></rect></svg> <!-- -->=<!-- --> <!-- -->[0.8, 0.7, 1, 1]</code> we’d expect a value close to 1 (dark!).</p>
<p>In contrast, for <code><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0.2, 0.5, 0.4, 0.7]</title><rect fill="#E3E3E3" x="0" y="0" width="1" height="1"></rect><rect fill="#989898" x="1" y="0" width="1" height="1"></rect><rect fill="#B1B1B1" x="0" y="1" width="1" height="1"></rect><rect fill="#656565" x="1" y="1" width="1" height="1"></rect></svg> <!-- -->=<!-- --> <!-- -->[0.2, 0.5, 0.4, 0.7]</code> we
expect something closer to 0 than to 1.</p>
<p>Let’s implement a neural network from our simple mental model. The goal of this
neural network is to take a grayscale 2x2 image and tell us how “dark” it is
where 0 is completely white <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0, 0, 0, 0]</title><rect fill="#FFFFFF" x="0" y="0" width="1" height="1"></rect><rect fill="#FFFFFF" x="1" y="0" width="1" height="1"></rect><rect fill="#FFFFFF" x="0" y="1" width="1" height="1"></rect><rect fill="#FFFFFF" x="1" y="1" width="1" height="1"></rect></svg>, and 1 is
completely black <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[1, 1, 1, 1]</title><rect fill="#000000" x="0" y="0" width="1" height="1"></rect><rect fill="#000000" x="1" y="0" width="1" height="1"></rect><rect fill="#000000" x="0" y="1" width="1" height="1"></rect><rect fill="#000000" x="1" y="1" width="1" height="1"></rect></svg>. We will initialize the
hidden layer with some random values at first, in Python:</p>
<pre class="language-python"><code class="language-python">input_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span>
<span class="token comment"># We randomly initialize the weights (values) for the hidden layer... We will</span>
<span class="token comment"># need to &quot;train&quot; to make these weights give us the output layers we desire. We</span>
<span class="token comment"># will cover that shortly!</span>
hidden_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.86</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.08</span><span class="token punctuation">]</span>

output_neuron <span class="token operator">=</span> <span class="token number">0</span>
<span class="token comment"># This is really matrix multiplication. We explicitly _do not_ use a</span>
<span class="token comment"># matrix/tensor, because they add overhead to understanding what happens here</span>
<span class="token comment"># unless you work with them every day--which you probably don&#x27;t. More on using</span>
<span class="token comment"># matrices later.</span>
<span class="token keyword">for</span> index<span class="token punctuation">,</span> input_neuron <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>input_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_neuron <span class="token operator">+=</span> input_neuron <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_neuron<span class="token punctuation">)</span>
<span class="token comment"># =&gt; 0.68</span>
</code></pre>
<p>Our neural network is giving us <code>model(<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0.2, 0.5, 0.4, 0.7]</title><rect fill="#E3E3E3" x="0" y="0" width="1" height="1"></rect><rect fill="#989898" x="1" y="0" width="1" height="1"></rect><rect fill="#B1B1B1" x="0" y="1" width="1" height="1"></rect><rect fill="#656565" x="1" y="1" width="1" height="1"></rect></svg>) =<!-- --> <!-- -->0.7</code> which is closer to ‘dark’ (1.0) than ‘light’ (0.0). When looking
at this rectangle as a human, we judge it to be more bright than dark, so we
were expecting something below 0.5!</p>
<div class="utils_articleNotice__5FGI8"><p>There’s a <a href="https://colab.research.google.com/drive/1YRp9k_ORH4wZMqXLNkc3Ir5w4B5f-8Pa?usp=sharing">notebook</a> with the final code available. You can make a copy and execute it there. For early versions of the code, such as the above, you can create a new cell at the beginning of the notebook and build up from there!</p></div>
<p>The only real thing we can change in our neural network in its current form is
the hidden layer’s values. How do we change the hidden layer values so that the
output neuron is close to 1 when the rectangle is dark, and close to 0 when it’s
light?</p>
<p>We could abandon this approach and just take the average of all the pixels. That
would work well! However, that’s not really the point of a neural net… We’ll
hit an impasse if we one day expand our model to try to implement
<code>recognize_letters_from_picture(img)</code> or <code>is_cat(img)</code>.</p>
<p>Fundamentally, a neural network is just a way to approximate any function. It’s
really hard to sit down and write <code>is_cat</code>, but the same technique we’re using
to implement <code>average</code> through a neural network can be used to implement
<code>is_cat</code>. This is called the <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximation theorem</a>: an
artificial neural network can approximate <em>any</em> function!</p>
<p>So, let’s try to teach our simple neural network to take the <code>average()</code> of the
pixels instead of explicitly telling it that that’s what we want! The idea of
this walkthrough example is to understand a neural net with very few values and
low complexity, otherwise it’s difficult to develop an intuition when we move to
1,000s of values and 10s of layers, as real neural networks have.</p>
<p>We can observe that if we <em>manually modify</em> all the hidden layer attributes to
<code>0.25</code>, our neural network is actually an average function!</p>
<pre class="language-python"><code class="language-python">input_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">]</span>
hidden_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.25</span><span class="token punctuation">,</span> <span class="token number">0.25</span><span class="token punctuation">,</span> <span class="token number">0.25</span><span class="token punctuation">,</span> <span class="token number">0.25</span><span class="token punctuation">]</span>

output_neuron <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">for</span> index<span class="token punctuation">,</span> input_neuron <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>input_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_neuron <span class="token operator">+=</span> input_neuron <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span>

<span class="token comment"># Two simple ways of calculating the same thing!</span>
<span class="token comment">#</span>
<span class="token comment"># 0.2 * 0.25 + 0.5 * 0.25 + 0.4 * 0.25 + 0.7 * 25 = 0.45</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_neuron<span class="token punctuation">)</span>
<span class="token comment"># Here, we divide by 4 to get the average instead of</span>
<span class="token comment"># multiplying each element.</span>
<span class="token comment">#</span>
<span class="token comment"># (0.2 + 0.5 + 0.4 + 0.7) / 4 = 0.45</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>input_layer<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">4</span><span class="token punctuation">)</span>
</code></pre>
<p><code>model(<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0.2, 0.5, 0.4, 0.7]</title><rect fill="#E3E3E3" x="0" y="0" width="1" height="1"></rect><rect fill="#989898" x="1" y="0" width="1" height="1"></rect><rect fill="#B1B1B1" x="0" y="1" width="1" height="1"></rect><rect fill="#656565" x="1" y="1" width="1" height="1"></rect></svg>) =<!-- --> <!-- -->0.45</code> sounds about right. The
rectangle is a little lighter than it’s dark.</p>
<p>But that was cheating! We only showed that we <em>can</em> implement <code>average()</code> by
simply changing the hidden layer’s values. But that won’t work if we try to implement
something more complicated. Let’s go back to our original hidden layer
initialized with random values:</p>
<pre class="language-python"><code class="language-python">hidden_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.86</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.08</span><span class="token punctuation">]</span>
</code></pre>
<p>How can we <em>teach</em> our neural network to implement <code>average</code>?</p>
<h2 id="training-our-neural-network">Training our Neural Network<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p>To teach our model, we need to create some training data. We’ll create some
rectangles and calculate their average:</p>
<pre class="language-python"><code class="language-python">rectangles <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
rectangle_average <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Generate a 2x2 rectangle [0.1, 0.8, 0.6, 1.0]</span>
    rectangle <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">round</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                 <span class="token builtin">round</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                 <span class="token builtin">round</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                 <span class="token builtin">round</span><span class="token punctuation">(</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    rectangles<span class="token punctuation">.</span>append<span class="token punctuation">(</span>rectangle<span class="token punctuation">)</span>
    <span class="token comment"># Take the _actual_ average for our training dataset!</span>
    rectangle_average<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">4</span><span class="token punctuation">)</span>
</code></pre>
<p>Brilliant, so we can now feed these to our little neural network and get a
result! Next step is for our neural network to adjust the values in the hidden
layer based on how its output compares with the actual average in the training
data. This is called our <code>loss</code> function: large loss, very wrong model; small
loss, less wrong model. We can use a standard measure called <a href="https://en.wikipedia.org/wiki/Mean_squared_error"><em>mean squared
error</em></a>:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Take the average of all the differences squared!</span>
<span class="token comment"># This calculates how &quot;wrong&quot; our predictions are.</span>
<span class="token comment"># This is called our &quot;loss&quot;.</span>
<span class="token keyword">def</span> <span class="token function">mean_squared_error</span><span class="token punctuation">(</span>actual<span class="token punctuation">,</span> expected<span class="token punctuation">)</span><span class="token punctuation">:</span>
    error_sum <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>actual<span class="token punctuation">,</span> expected<span class="token punctuation">)</span><span class="token punctuation">:</span>
        error_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> b<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>
    <span class="token keyword">return</span> error_sum <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>actual<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># =&gt; 1.0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># =&gt; 4.0</span>
</code></pre>
<p>Now we can implement <code>train()</code>:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_neuron <span class="token operator">=</span> <span class="token number">0.</span>
    <span class="token keyword">for</span> index<span class="token punctuation">,</span> input_neuron <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_neuron <span class="token operator">+=</span> input_neuron <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
    <span class="token keyword">return</span> output_neuron

<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> rectangle <span class="token keyword">in</span> rectangles<span class="token punctuation">:</span>
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>
      outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
  <span class="token keyword">return</span> outputs

hidden_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.86</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.08</span><span class="token punctuation">]</span>
outputs <span class="token operator">=</span> train<span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># [1.472, 0.7, 1.369, 0.8879, 1.392, 1.244, 0.644, 1.1179, 0.474, 1.54]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rectangle_average<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># [0.575, 0.45, 0.549, 0.35, 0.525, 0.475, 0.425, 0.65, 0.4, 0.575]</span>
mean_squared_error<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> rectangle_average<span class="token punctuation">)</span>
<span class="token comment"># 0.4218</span>
</code></pre>
<p>A good mean squared error is close to 0. Our model isn’t very good. But! We’ve
got the skeleton of a feedback loop in place for updating the hidden layer.</p>
<h3 id="updating-the-hidden-layer-with-gradient-descent">Updating the Hidden Layer with Gradient Descent<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h3>
<p>Now what we need is a way to update the hidden layer in response to the mean
squared error / loss. We need to <em>minimize</em> the value of this function:</p>
<pre class="language-python"><code class="language-python">mean_squared_error<span class="token punctuation">(</span>
  train<span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>
  rectangle_average
<span class="token punctuation">)</span>
</code></pre>
<p>As noted earlier, the only thing we can really change here are the weights in
the hidden layer. How can we possibly know which weights will minimize this
function?</p>
<p>We could randomize the weights, calculate the loss (how wrong the model is,
in our case, with mean squared error), and then save the best ones we see after
some period of time.</p>
<p>We could possibly speed this up. If we have good weights, we could try to add
some random numbers to those. See if loss improves. This could work, but it
sounds slow… and likely to get stuck in some local maxima and not give a very
good result. And it’s trouble scaling this to 1,000s of weights…</p>
<p>Instead of embarking on this ad-hoc randomization mess, it turns out that
there’s a method called <em>gradient descent</em> to minimize the value of a function!
Gradient descent builds on a bit of calculus that you may not have touched on
since high school. We won’t go into depth here, but try to introduce <em>just</em>
enough that you understand the concept. <sup><a href="/napkin/neural-net#user-content-fn-3blue1brown">3</a></sup></p>
<p>Let’s try to understand gradient descent. Consider some random function whose
graph might look like this:</p>
<figure><img alt="Graph of a random function with some irregular shapes" title="Graph of a function with an irregular curve with a local and global minimum." sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction.png&amp;w=3840&amp;q=75" width="775" height="485" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 775 485&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFtaAABbWgFGH/T3AAAAL0lEQVR4nGNYtGgRAxgcevmfYdPX/8d+/z/w9v+Sl/8ZFi3cOn/v3i337i7YcgoAlZ4XzUe5RowAAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)"/><figcaption>Graph of a function with an irregular curve with a local and global minimum.</figcaption></figure>
<p>How do we write code to find the minimum, the deepest (second) valley, of this function?</p>
<p>Let’s say that we’re at <code>x=1</code> and we know the <em>slope</em> of the function at this
point. The slope is “how fast the function grows at this very point.” You may
remember this as <em>the derivative</em>. The slope at <code>x=1</code> might be <code>-1.5</code>. This
means that every time we increase <code>x += 1</code>, it results in <code>y -= 1.5</code>. We’ll go
into how you figure out the slope in a bit, let’s focus on the concept first.</p>
<figure><img alt="Graph function with some slope or derivative" sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Ffunction-with-slope.png&amp;w=3840&amp;q=75" width="1934" height="1256" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1934 1256&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFtaAABbWgFGH/T3AAAAL0lEQVR4nGOYNm0aAxhsuPGf4datZ6fv/j/0+//iN/8ZLmw68PDs2YXHb89Yvx4AkEUXyxb02LEAAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)"/></figure>
<p>The idea of gradient descent is that since we know the value of our function,
<code>y</code>, is decreasing as we increase <code>x</code>, we can increase <code>x</code> proportionally to the
slope. In other words, if we increase <code>x</code> by the slope, we step towards the
valley by <code>1.5</code>.</p>
<p>Let’s take that step of <code>x += 1.5</code>:</p>
<figure><img alt="Overshooting in gradient descent" sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fgradient-descent-overshoot.png&amp;w=3840&amp;q=75" width="1934" height="1256" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1934 1256&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFtaAABbWgFGH/T3AAAALklEQVR4nGPI6OlmAIOVl/4znLj68P+n/4ce/1/18z/D8VU7bhw50rXvSv/SpQCFDxcsf747hwAAAABJRU5ErkJggg==&#x27;/%3E%3C/svg%3E&quot;)"/></figure>
<p>Ugh, turned out that we stepped <em>too</em> far, past this valley! If we repeat the
step, we’ll land somewhere on the left side of the valley, to then bounce back
on the right side. We might <em>never</em> land in the bottom of the valley. Bummer.
Either way, this isn’t the <em>global minimum</em> of the function. We return to that
in a moment!</p>
<p>We can fix the overstepping easily by taking smaller steps. Perhaps we should’ve
stepped by just <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.1</mn><mo>∗</mo><mn>1.5</mn><mo>=</mo><mn>0.15</mn></mrow><annotation encoding="application/x-tex">0.1 * 1.5 = 0.15</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1.5</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.15</span></span></span></span></span> instead. That would’ve smoothly landed us at
the bottom of the valley. That multiplier, <code>0.1</code>, is called the <em>learning rate</em>
in gradient descent.</p>
<figure><img alt="Minimum of function with gradient descent" sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fminimum.png&amp;w=3840&amp;q=75" width="1934" height="1256" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 1934 1256&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFteAABbXgG54nPlAAAAL0lEQVR4nGNgYWHp+n8q9+gRBp80Bj1LF9XSBQx1ExkyOxh4eaX4VVQYRFQYxMUB74YJ3gaVuHkAAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)"/></figure>
<p>But hang on, that’s not actually the minimum of the function. See that valley to
the right? That’s the <em>actual</em> global minimum. If our initial <code>x</code> value had been
e.g. 3, we might have found the global minimum instead of our local minimum.</p>
<p>Finding the global minimum of a function is <em>hard</em>. Gradient descent will give
us <em>a minimum</em>, but not <em>the minimum</em>. Unfortunately, it turns out it’s the best
weapon we have at our disposal. Especially when we have big, complicated
functions (like a neural net with millions of neurons). Gradient descent will
not always find the global minimum, but something <em>pretty</em> good.</p>
<p>This method of using the slope/derivative generalizes. For example, consider
optimizing a function in three-dimensions. We can visualize the gradient descent
method here as <em>rolling a ball to the lowest point.</em> A big neural network is
1000s of dimensions, but gradient descent still works to minimize the loss!</p>
<figure><img alt="Depicts a 3-dimensional graph, if we do gradient descent on this we might imagine it as rolling a ball down the hill." title="Depicts a 3-dimensional graph, if we do gradient descent on this we might imagine it as rolling a ball down the hill." sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fdescent-3d.png&amp;w=3840&amp;q=75" width="760" height="624" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 760 624&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAMklEQVR4nAEnANj/APb29o1WeWxUd7u6ywDIz7YYLQkACCSBgpsA6vDlr9Oq+vv7+/z7mscW6F9NiW0AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)"/><figcaption>Depicts a 3-dimensional graph, if we do gradient descent on this we might imagine it as rolling a ball down the hill.</figcaption></figure>
<h2 id="finalizing-our-neural-network-from-scratch">Finalizing our Neural Network from scratch<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p>Let’s summarize where we are:</p>
<ul>
<li>We can implement a simple neural net: <code>model()</code>.</li>
<li>Our neural net can figure out how <em>wrong</em> it is for a training set: <code>loss(train())</code>.</li>
<li>We have a method, <em>gradient descent</em>, for tuning our hidden layer’s weights
for the minimum loss. I.e. we have a method to adjust those four random values
in our hidden layer to take a <em>better</em> average as we iterate through the
training data.</li>
</ul>
<p>Now, let’s implement gradient descent and see if we can make our neural net
learn to take the average grayscale of our small rectangles:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_neuron <span class="token operator">=</span> <span class="token number">0.</span>
    <span class="token keyword">for</span> index<span class="token punctuation">,</span> input_neuron <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_neuron <span class="token operator">+=</span> input_neuron <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
    <span class="token keyword">return</span> output_neuron

<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> rectangle <span class="token keyword">in</span> rectangles<span class="token punctuation">:</span>
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>
      outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

  mean_squared_error<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> rectangle_average<span class="token punctuation">)</span>

  <span class="token comment"># We go through all the weights in the hidden layer. These correspond to all</span>
  <span class="token comment"># the weights of the function we&#x27;re trying to minimize the value of: our</span>
  <span class="token comment"># model, respective of its loss (how wrong it is).</span>
  <span class="token comment"># </span>
  <span class="token comment"># For each of the weights, we want to increase/decrease it based on the slope.</span>
  <span class="token comment"># Exactly like we showed in the one-weight example above with just x. Now</span>
  <span class="token comment"># we just have 4 values instead of 1! Big models have billions.</span>
  <span class="token keyword">for</span> index<span class="token punctuation">,</span> _ <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>
    <span class="token comment"># But... how do we get the slope/derivative?!</span>
    hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">.</span>slope

  <span class="token keyword">return</span> outputs

hidden_layer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.86</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.08</span><span class="token punctuation">]</span>
train<span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>
</code></pre>
<h3 id="automagically-computing-the-slope-of-a-function-with-autograd">Automagically computing the slope of a function with <code>autograd</code><svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h3>
<p>The missing piece here is to figure out the <code>slope()</code> after we’ve gone through
our training set. Figuring out the slope/derivative at a certain point is
tricky. It involves a fair bit of math. I am not going to go into the math of
calculating derivatives. Instead, we’ll do what all the machine learning
libraries do: automatically calculate it. <sup><a href="/napkin/neural-net#user-content-fn-nielsen">4</a></sup></p>
<p>Minimizing the loss of a function is absolutely fundamental to machine learning.
The functions (neural networks) are <em>so</em> complicated that manually sitting down
to figure out the derivative like you might’ve done in high school is not
feasible. It’s the mathematical equivalent of writing assembly to implement a
website.</p>
<p>Let’s show one simple example of finding the derivative of a function, before we
let the computers do it all for us. If we have <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x) = x^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>, then you might
remember from calculus classes that the derivative is <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn><mi>x</mi></mrow><annotation encoding="application/x-tex">f&#x27;(x) = 2x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span><span class="mord mathnormal">x</span></span></span></span></span>. In other
words, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>’s slope at any point is <code>2x</code>, telling us it’s increasing
non-linearly. Well that’s exactly how we understand <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>, perfect! This means
that for <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">x = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span> the slope is <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">4</span></span></span></span></span>.</p>
<p>With the basics in order, we can use an <code>autograd</code> package to avoid the messy
business of computing our own derivatives. <code>autograd</code> is an <em>automatic
differentiation engine</em>. <em>grad</em> stands for <em>gradient</em>, which we can think of as the
derivative/slope of a function with more than one parameter.</p>
<p>It’s best to show how it works by using our example from before:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch

<span class="token comment"># A tensor is a matrix in PyTorch. It is the fundamental data-structure of neural</span>
<span class="token comment"># networks. Here we say PyTorch, please keep track of the gradient/derivative</span>
<span class="token comment"># as I do all kinds of things to the parameter(s) of this tensor.</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># At this point we&#x27;re applying our function f(x) = x^2.</span>
y <span class="token operator">=</span> x <span class="token operator">**</span> <span class="token number">2</span>

<span class="token comment"># This tells `autograd` to compute the derivative values for all the parameters</span>
<span class="token comment"># involved. Backward is neural network jargon for this operation, which we&#x27;ll</span>
<span class="token comment"># explain momentarily.</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># And show us the lovely gradient/derivative, which is 4! Sick.</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token comment"># =&gt; 4</span>
</code></pre>
<p><code>autograd</code> is the closest to magic we get. I could do the most ridiculous stuff
with this tensor, and it’ll keep track of all the math operations applied and
have the ability to compute the derivative. We won’t go into how. Partly because
I don’t know how, and this post is long enough.</p>
<p>Just to convince you of this, we can be a little cheeky and do a bunch of random
stuff. I’m trying to really hammer this home, because this is what confused me
the most when learning about neural networks. It wasn’t obvious to me that a
neural network, including executing the loss function on the whole training set,
is <em>just</em> a function, and however complicated, we can still take the derivative
of it and use gradient descent. Even if it’s so many dimensions that it can’t be
neatly visualized as a ball rolling down a hill.</p>
<p><code>autograd</code> doesn’t complain as we add complexity and will still calculate the
gradients. In this example we’ll even use a matrix/tensor with a few more elements and
calculate an average (like our loss function <code>mean_squared_error</code>), which is the
kind of thing we’ll calculate the gradients for in our neural network:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> random
<span class="token keyword">import</span> torch

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x

<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    choice <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> choice <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> y <span class="token operator">**</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> choice <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> y<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> choice <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> y<span class="token punctuation">.</span>atanh<span class="token punctuation">(</span><span class="token punctuation">)</span>

y <span class="token operator">=</span> y<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># This walks &quot;backwards&quot; y all the way to the parameters to</span>
<span class="token comment"># calculate the derivates / gradient! Pytorch keeps track of a graph of all the</span>
<span class="token comment"># operations.</span>
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># And here are how quickly the function is changing with respect to these</span>
<span class="token comment"># parameters for our randomized function.</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token comment"># =&gt; tensor([0.0157, 0.0431, 0.6338, 0.0028])</span>
</code></pre>
<p>Let’s use <code>autograd</code> for our neural net and then run it against our square from
earlier <code>model(<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0.2, 0.5, 0.4, 0.7]</title><rect fill="#E3E3E3" x="0" y="0" width="1" height="1"></rect><rect fill="#989898" x="1" y="0" width="1" height="1"></rect><rect fill="#B1B1B1" x="0" y="1" width="1" height="1"></rect><rect fill="#656565" x="1" y="1" width="1" height="1"></rect></svg>) =<!-- --> <!-- -->0.45</code>:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch <span class="token keyword">as</span> torch

<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_neuron <span class="token operator">=</span> <span class="token number">0.</span>
    <span class="token keyword">for</span> index<span class="token punctuation">,</span> input_neuron <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_neuron <span class="token operator">+=</span> input_neuron <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
    <span class="token keyword">return</span> output_neuron

<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> rectangle <span class="token keyword">in</span> rectangles<span class="token punctuation">:</span>
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>
      outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

  <span class="token comment"># How wrong were we? Our &#x27;loss.&#x27;</span>
  error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> rectangle_average<span class="token punctuation">)</span>

  <span class="token comment"># Calculate the gradient (the derivate for all our weights!)</span>
  <span class="token comment"># This walks &quot;backwards&quot; from the error all the way to the weights to</span>
  <span class="token comment"># calculate them</span>
  error<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token comment"># Now let&#x27;s go update the weights in our hidden layer per our gradient.</span>
  <span class="token comment"># This is what we discussed before: we want to find the valley of this</span>
  <span class="token comment"># four-dimensional space/four-weight function. This is gradient descent!</span>
  <span class="token keyword">for</span> index<span class="token punctuation">,</span> _ <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>
    <span class="token comment"># hidden_layer.grad is something like [0.7070, 0.6009, 0.6840, 0.5302]</span>
    hidden_layer<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> hidden_layer<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span>

  <span class="token comment"># We have to tell `autograd` that we&#x27;ve just finished an epoch to reset.</span>
  <span class="token comment"># Otherwise it&#x27;d calculate the derivative from multiple epochs.</span>
  hidden_layer<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> error

<span class="token comment"># We use tensors now, but we just use them as if they were normal lists.</span>
<span class="token comment"># We only use them so we can get the gradients.</span>
hidden_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.86</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.08</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.4</span><span class="token punctuation">,</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># =&gt; 0.6840000152587891</span>

train<span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>

<span class="token comment"># The hidden layer&#x27;s weights are nudging closer to [0.25, 0.25, 0.25, 0.25]!</span>
<span class="token comment"># They are now [ 0.9093,  0.3399,  0.7916, -0.1330]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;After: </span><span class="token interpolation"><span class="token punctuation">{</span>model<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.4</span><span class="token punctuation">,</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token comment"># =&gt; 0.5753424167633057</span>
<span class="token comment"># The average of this rectangle is 0.45, closer... but not there yet</span>
</code></pre>
<p>This blew my mind the first time I did this. Look at that. It’s optimizing the
hidden layer for all weights in the right direction! We’re expecting them all
to nudge towards <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.25</mn></mrow><annotation encoding="application/x-tex">0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.25</span></span></span></span></span> to implement <code>average()</code>. We haven’t told it <em>anything</em>
about average, we’ve just told it how wrong it is through the loss.</p>
<div class="utils_articleNotice__5FGI8"><p>It’s important to understand how <code>hidden_layer.grad</code> is set here. The
hidden layer is instantiated as a tensor with an argument telling Pytorch to
keep track of all operations made to it. This allows us to later call <code>backward()</code> on a future tensor that derives from the hidden layer,
in this case, the <code>error</code> tensor, which is further derived from the
<code> outputs</code> tensor. You can read more in <a href="https://pytorch.org/docs/1.9.1/generated/torch.Tensor.backward.html">the documentation</a></p></div>
<p><em>But</em>, the hidden layer isn’t all <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.25</mn></mrow><annotation encoding="application/x-tex">0.25</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.25</span></span></span></span></span> quite yet, as we expect for it to
implement <code>average</code>. So how do we get them to that? Well, let’s try to repeat
the gradient descent process 100 times and see if we’re getting even better!</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># An epoch is a training pass over the full data set!</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
   error <span class="token operator">=</span> train<span class="token punctuation">(</span>rectangles<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span>
   <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string">, Error: </span><span class="token interpolation"><span class="token punctuation">{</span>error<span class="token punctuation">}</span></span><span class="token string">, Layer: </span><span class="token interpolation"><span class="token punctuation">{</span>hidden_layer<span class="token punctuation">.</span>data<span class="token punctuation">}</span></span><span class="token string">\n\n&quot;</span></span><span class="token punctuation">)</span>
   <span class="token comment"># </span>
   <span class="token comment">#  Epoch: 99, Error: 0.0019292341312393546, Layer: tensor([0.3251, 0.2291, 0.3075, 0.1395])</span>


<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.4</span><span class="token punctuation">,</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># =&gt; 0.4002</span>
</code></pre>
<p>Pretty close, but not quite there. I ran it for <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>300</mn></mrow><annotation encoding="application/x-tex">300</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">300</span></span></span></span></span> times (an iteration over
the full training set is referred to as an epoch, so 300 epochs) instead, and
then I got:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.4</span><span class="token punctuation">,</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># Epoch: 299, Error: 1.8315197394258576e-06, Layer: tensor([0.2522, 0.2496, 0.2518, 0.2465])</span>
<span class="token comment"># tensor(0.4485, grad_fn=&lt;AddBackward0&gt;)</span>
</code></pre>
<p>Boom! Our neural net has <em>almost</em> learned to take the average, off by just a
scanty <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.002</mn></mrow><annotation encoding="application/x-tex">0.002</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.002</span></span></span></span></span>. If we fine-tuned the learning rate and number of epochs we could
probably get it there, but I’m happy with this. <code>model(<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 2 2" width="18" height="18" style="position:relative;top:3px"><title>[0.2, 0.5, 0.4, 0.7]</title><rect fill="#E3E3E3" x="0" y="0" width="1" height="1"></rect><rect fill="#989898" x="1" y="0" width="1" height="1"></rect><rect fill="#B1B1B1" x="0" y="1" width="1" height="1"></rect><rect fill="#656565" x="1" y="1" width="1" height="1"></rect></svg>) =<!-- --> <!-- -->0.448</code>:</p>
<p>That’s it. That’s your first neural net:</p>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo stretchy="false">(</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>g</mi><mi>l</mi><mi>e</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>a</mi><mi>v</mi><mi>g</mi><mo stretchy="false">(</mo><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>g</mi><mi>l</mi><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">model(rectangle) \approx avg(rectangle)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mopen">(</span><span class="mord mathnormal">rec</span><span class="mord mathnormal">t</span><span class="mord mathnormal">an</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">e</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em">vg</span><span class="mopen">(</span><span class="mord mathnormal">rec</span><span class="mord mathnormal">t</span><span class="mord mathnormal">an</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">e</span><span class="mclose">)</span></span></span></span></span></div>
<h2 id="ok-so-you-just-implemented-the-most-complicated-average-function-ive-ever-seen">OK, so you just implemented the most complicated <code>average</code> function I’ve ever seen…<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p>Sure did. The thing is, that if we adjusted it for looking for cats, it’s the
least complicated <code>is_cat</code> you’ll ever see. Because our neural network could
implement that too by changing the training data. Remember, a neural network
with enough neurons can approximate <em>any function</em>. You’ve just learned all the
building blocks to do it. We just started with the simplest possible example.</p>
<p>If you give the hidden layer some more neurons, this neural net will be able to
recognize <a href="http://yann.lecun.com/exdb/mnist/">handwritten numbers</a> with decent accuracy (possible fun
exercise for you, see bottom of article), like this one:</p>
<figure><img alt="An upscaled version of a handdrawn 3 from the 28x28 MNIST dataset." title="An upscaled version of a handdrawn 3 from the 28x28 MNIST dataset." sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Fmnist-sample.png&amp;w=3840&amp;q=75" width="200" height="200" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 200 200&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAIAAAAmkwkpAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAMklEQVR4nGNgYGDw8PAICQlhgICenp7///+DWMzMzCUlJbt37wZx2NjYKisrLS0tgRwAO/EMNDi6S+4AAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;)"/><figcaption>An upscaled version of a handdrawn 3 from the 28x28 MNIST dataset.</figcaption></figure>
<h3 id="activation-functions">Activation Functions<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h3>
<p>To be truly powerful, there is one paramount modification we have to make to our
neural net. Above, we were implementing the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">average</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord mathnormal" style="margin-right:0.02778em">er</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mord mathnormal">e</span></span></span></span></span> function. However, were
our neural net to implement <code>which_digit(png)</code> or <code>is_cat(jpg)</code> then it wouldn’t work.</p>
<p>Recognizing handwritten digits isn’t a <em>linear</em> function, like <code>average()</code>. It’s
non-linear. It’s a crazy function, with a crazy shape (unlike a linear
function). To create crazy functions with crazy shapes, we have to introduce a
non-linear component to our neural network. This is called an <em>activation</em>
function. It can be e.g. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>L</mi><mi>u</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ReLu(x) = max(0, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mord mathnormal">e</span><span class="mord mathnormal">Lu</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>. There are many kinds of
<a href="https://en.wikipedia.org/wiki/Activation_function">activation functions</a> that are good for different things.
<sup><a href="/napkin/neural-net#user-content-fn-activation">5</a></sup></p>
<figure><img alt="" sizes="(min-width: 36rem) 36rem 100vw" srcSet="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=%2Fimages%2Fnapkin%2Fproblem-17-neural-nets%2Frelu.png&amp;w=3840&amp;q=75" width="400" height="333" decoding="async" data-nimg="1" loading="lazy" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http%3A//www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 400 333&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage preserveAspectRatio=&#x27;none&#x27; filter=&#x27;url(%23b)&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; height=&#x27;100%25&#x27; width=&#x27;100%25&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAMElEQVR4nGMwMDI0MTVpbGjUMtBlmDhhwv///xcuXxYVH8tgYGRoZGmupqPV29sLABM2DkTEfNKgAAAAAElFTkSuQmCC&#x27;/%3E%3C/svg%3E&quot;)"/></figure>
<p>We can apply this simple operation to our neural net:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">,</span> hidden_layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_neuron <span class="token operator">=</span> <span class="token number">0.</span>
    <span class="token keyword">for</span> index<span class="token punctuation">,</span> input_neuron <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>rectangle<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output_neuron <span class="token operator">+=</span> input_neuron <span class="token operator">*</span> hidden_layer<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> output_neuron<span class="token punctuation">)</span>
</code></pre>
<p>Now, we only have a single neuron/weight… that isn’t much. Good models have
100s, and the biggest models like GPT-3 have billions. So this won’t recognize
many digits or cats, but you can easily add more weights!</p>
<h3 id="matrices">Matrices<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h3>
<p>The core operation in our model, the for loop, is matrix multiplication. We could
rewrite it to use them instead, e.g. <code>rectangle @ hidden_layer</code>. PyTorch will
then do the exact same thing. Except, it’ll now execute in C-land. And if you
have a GPU and pass some extra weights, it’ll execute on a GPU, which is even
faster. When doing any kind of deep learning, you want to avoid writing any
Python loops. They’re just too slow. If you ran the code above for the 300
epochs, you’ll see that it takes minutes to complete. I left matrices out of it
to simplify the explanation as much as possible. There’s plenty going on without
them.</p>
<h2 id="next-steps-to-implement-your-own-neural-net-from-scratch">Next steps to implement your own neural net from scratch<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<p>Even if you’ve carefully read through this article, you won’t fully grasp it
yet until you’ve had your own hands on it. Here are some suggestions on where to
go from here, if you’d like to move beyond the basic understanding you have now:</p>
<ol>
<li>Get the <a href="https://colab.research.google.com/drive/1YRp9k_ORH4wZMqXLNkc3Ir5w4B5f-8Pa?usp=sharing">notebook</a> running and study the code</li>
<li>Change it to far larger rectangles, e.g. 100x100</li>
<li>Add biases in addition to the weights. A model doesn’t just have
weights that are multiplied onto the inputs, but also biases that are added
(<code>+</code>) onto the inputs in each layer.</li>
<li>Rewrite the model to use <a href="https://pytorch.org/docs/stable/tensors.html">PyTorch tensors</a> for matrix operations, as
described in the previous section.</li>
<li>Add 1-2 more layers to the model. Try to have them have different sizes.</li>
<li>Change the tensors to run on GPU (see the <a href="https://pytorch.org/docs/stable/notes/cuda.html">PyTorch
documentation</a>) and see the
performance speed up! Increase the size of the training set and rectangles to
<em>really</em> be able to tell the difference. Make sure you change <code>Runtime &gt; Change Runtime Type</code> in Collab to run on a GPU.</li>
<li>This is a difficult step that will likely take a while, but it’ll be well
worth it: Adapt the code to recognize handwritten letters from the <a href="https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz">MNIST
dataset</a> dataset. You’ll need to use <a href="https://pillow.readthedocs.io/en/stable/"><code>pillow</code></a> to turn
the pixels into a large 1-dimensional tensor as the input layer, as well as a
non-linear activation function like <code>Sigmoid</code> or <code>ReLU</code>. Use <a href="http://neuralnetworksanddeeplearning.com/">Nielsen’s
book</a> as a reference if you get stuck, which does exactly this.</li>
</ol>
<p>I thoroughly hope you enjoyed this walkthrough of a neural net from scratch! In
a future issue we’ll use the mental model we’ve built up here to do some napkin
math on expected performance on training and using neural nets.</p>
<p><em>Thanks to <a href="https://www.vegardstikbakke.com/">Vegard Stikbakke</a>, <a href="https://www.flyingcroissant.ca/">Andrew Bugera</a> and <a href="https://thundergolfer.com/">Jonathan
Belotti</a> for providing valuable feedback on drafts of this article.</em></p>
<section data-footnotes="true" class="footnotes"><h2 id="footnote-label" class="sr-only">Footnotes<svg class="autolink-svg" xmlns="http://www.w3.org/2000/svg" width="18" height="18" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></h2>
<ol>
<li id="user-content-fn-google">
<p>This is a good example of <a href="/peak-complexity">Peak Complexity</a>.
The existing phrase-based translation model was iteratively improved with
increasing complexity, distributed systems to look up five-word phrases
frequencies, etc. The complexity required to improve the model 1% was becoming
astronomical. A good hint you need a paradigm shift to reset the complexity.
Deep Learning provided that complexity reset for the translation model. <a href="/napkin/neural-net#user-content-fnref-google">↩</a></p>
</li>
<li id="user-content-fn-gpt3">
<p>GPT-3 has ~175 billion weights. The human brain has ~86 billion. Of
course, you cannot technically compare an artificial neuron to a real one.
Why? I don’t know. I reserve that it remains an interesting question. <a href="https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game">It’s
estimated</a> that it cost in the double-digit millions to train it. <a href="/napkin/neural-net#user-content-fnref-gpt3">↩</a></p>
</li>
<li id="user-content-fn-3blue1brown">
<p>There’s a brilliant <a href="https://www.youtube.com/watch?v=aircAruvnKk">Youtube series</a> that’ll go
into more depth on the math than I do in this article. This article
accompanies the video nicely, as the video doesn’t go into the implementation. <a href="/napkin/neural-net#user-content-fnref-3blue1brown">↩</a></p>
</li>
<li id="user-content-fn-nielsen">
<p>There’s a great, <a href="http://neuralnetworksanddeeplearning.com/">short e-book</a> on implementing a neural
network from scratch available that goes into far more detail on computing the
derivative from scratch. Despite this existing, I still decided to do this
write-up because calculating the slope manually takes up a lot of time and
complexity. I wanted to teach it from scratch without going into those
details. <a href="/napkin/neural-net#user-content-fnref-nielsen">↩</a></p>
</li>
<li id="user-content-fn-activation">
<p>I found this pretty strange when I learned about neural networks.
We can use a bunch of random non-linear function and our neural network
works… better? The answer is yes! The complicated answer I am not
knowledgeable enough to offer… If you write your own handwritten MNIST
neural net (as suggested at the end of the article), you can see for yourself
by adding/removing a non-linear function and looking at the loss. <a href="/napkin/neural-net#user-content-fnref-activation">↩</a></p>
</li>
</ol>
</section><div class="utils_notice__8AIwY">Subscribe through email,<!-- --> <a href="/atom.xml">RSS</a> <!-- -->or<!-- --> <a href="https://twitter.com/Sirupsen" title="Twitter" target="_blank" rel="noopener noreferrer">Twitter</a> <!-- -->to new articles!<form action="https://buttondown.email/api/emails/embed-subscribe/computer-napkins" method="post" id="subscribe" target="popupwindow" class="embeddable-buttondown-form"><input type="email" name="email" id="bd-email" placeholder="you@gmail.com"/><input type="hidden" name="embed" value="1"/><input type="submit" value="Subscribe"/></form><p> <!-- -->2,873<!-- --> <!-- -->subscribers</p></div><p><i>You might also like...</i></p><ul><li class="utils_blueLinks__C_dpi"><a href="/napkin/problem-10-mysql-transactions-per-second">MySQL transactions per second vs fsyncs per second</a></li><li class="utils_blueLinks__C_dpi"><a href="/napkin/problem-14-using-checksums-to-verify">Using checksums to verify syncing 100M database records</a></li><li class="utils_blueLinks__C_dpi"><a href="/shitlists">Shitlist Driven Development</a></li><li class="utils_blueLinks__C_dpi"><a href="/napkin/problem-15">Increase HTTP Performance by Fitting In the Initial TCP Slow Start Window</a></li><li class="utils_blueLinks__C_dpi"><a href="/napkin/problem-9">Inverted Index Performance and Merkle Tree Syncronization</a></li></ul></article><div class="layout_backToHome__9sjx_"><a href="/static">← Back to Blog</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"id":"napkin/neural-net","bodyMDX":{"compiledSource":"/*@jsxRuntime automatic @jsxImportSource react*/\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = Object.assign({}, _provideComponents(), props.components);\n  return MDXLayout ? _jsx(MDXLayout, Object.assign({}, props, {\n    children: _jsx(_createMdxContent, {})\n  })) : _createMdxContent();\n  function _createMdxContent() {\n    const _components = Object.assign({\n      nav: \"nav\",\n      ol: \"ol\",\n      li: \"li\",\n      a: \"a\",\n      p: \"p\",\n      em: \"em\",\n      sup: \"sup\",\n      h2: \"h2\",\n      svg: \"svg\",\n      path: \"path\",\n      img: \"img\",\n      ul: \"ul\",\n      strong: \"strong\",\n      code: \"code\",\n      pre: \"pre\",\n      span: \"span\",\n      h3: \"h3\",\n      math: \"math\",\n      semantics: \"semantics\",\n      mrow: \"mrow\",\n      mn: \"mn\",\n      mo: \"mo\",\n      annotation: \"annotation\",\n      mi: \"mi\",\n      msup: \"msup\",\n      div: \"div\",\n      section: \"section\"\n    }, _provideComponents(), props.components), {Rectangle, Notice} = _components;\n    if (!Notice) _missingMdxReference(\"Notice\", true);\n    if (!Rectangle) _missingMdxReference(\"Rectangle\", true);\n    return _jsxs(_Fragment, {\n      children: [_jsx(_components.nav, {\n        className: \"toc\",\n        children: _jsxs(_components.ol, {\n          className: \"toc-level toc-level-1\",\n          children: [_jsx(_components.li, {\n            className: \"toc-item toc-item-h2\",\n            children: _jsx(_components.a, {\n              className: \"toc-link toc-link-h2\",\n              href: \"#mental-model-for-a-neural-net-building-one-from-scratch\",\n              children: \"Mental Model for a Neural Net: Building one from scratch\"\n            })\n          }), _jsxs(_components.li, {\n            className: \"toc-item toc-item-h2\",\n            children: [_jsx(_components.a, {\n              className: \"toc-link toc-link-h2\",\n              href: \"#training-our-neural-network\",\n              children: \"Training our Neural Network\"\n            }), _jsx(_components.ol, {\n              className: \"toc-level toc-level-2\",\n              children: _jsx(_components.li, {\n                className: \"toc-item toc-item-h3\",\n                children: _jsx(_components.a, {\n                  className: \"toc-link toc-link-h3\",\n                  href: \"#updating-the-hidden-layer-with-gradient-descent\",\n                  children: \"Updating the Hidden Layer with Gradient Descent\"\n                })\n              })\n            })]\n          }), _jsxs(_components.li, {\n            className: \"toc-item toc-item-h2\",\n            children: [_jsx(_components.a, {\n              className: \"toc-link toc-link-h2\",\n              href: \"#finalizing-our-neural-network-from-scratch\",\n              children: \"Finalizing our Neural Network from scratch\"\n            }), _jsx(_components.ol, {\n              className: \"toc-level toc-level-2\",\n              children: _jsx(_components.li, {\n                className: \"toc-item toc-item-h3\",\n                children: _jsx(_components.a, {\n                  className: \"toc-link toc-link-h3\",\n                  href: \"#automagically-computing-the-slope-of-a-function-with-autograd\",\n                  children: \"Automagically computing the slope of a function with autograd\"\n                })\n              })\n            })]\n          }), _jsxs(_components.li, {\n            className: \"toc-item toc-item-h2\",\n            children: [_jsx(_components.a, {\n              className: \"toc-link toc-link-h2\",\n              href: \"#ok-so-you-just-implemented-the-most-complicated-average-function-ive-ever-seen\",\n              children: \"OK, so you just implemented the most complicated average function I’ve ever seen…\"\n            }), _jsxs(_components.ol, {\n              className: \"toc-level toc-level-2\",\n              children: [_jsx(_components.li, {\n                className: \"toc-item toc-item-h3\",\n                children: _jsx(_components.a, {\n                  className: \"toc-link toc-link-h3\",\n                  href: \"#activation-functions\",\n                  children: \"Activation Functions\"\n                })\n              }), _jsx(_components.li, {\n                className: \"toc-item toc-item-h3\",\n                children: _jsx(_components.a, {\n                  className: \"toc-link toc-link-h3\",\n                  href: \"#matrices\",\n                  children: \"Matrices\"\n                })\n              })]\n            })]\n          }), _jsx(_components.li, {\n            className: \"toc-item toc-item-h2\",\n            children: _jsx(_components.a, {\n              className: \"toc-link toc-link-h2\",\n              href: \"#next-steps-to-implement-your-own-neural-net-from-scratch\",\n              children: \"Next steps to implement your own neural net from scratch\"\n            })\n          })]\n        })\n      }), _jsx(_components.p, {\n        children: \"In this edition of Napkin Math, we’ll invoke the spirit of the Napkin Math\\nseries to establish a mental model for how a neural network works by building\\none from scratch. In a future issue we will do napkin math on performance, as\\nestablishing the first-principle understanding is plenty of ground to cover for\\ntoday!\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Neural nets are increasingly dominating the field of machine learning / artificial\\nintelligence: the most sophisticated models for computer vision (e.g. CLIP),\\nnatural language processing (e.g. GPT-3), translation (e.g. Google Translate),\\nand more are based on neural nets. When these artificial neural nets reach some\\narbitrary threshold of neurons, we call it \", _jsx(_components.em, {\n          children: \"deep learning\"\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"A visceral example of Deep Learning’s unreasonable effectiveness comes from\\n\", _jsx(_components.a, {\n          href: \"https://www.listennotes.com/podcasts/the-twiml-ai/systems-and-software-for-xolUkM23Gb0/\",\n          children: \"this interview\"\n        }), \" with Jeff Dean who leads AI at Google. He explains how\\n500 lines of Tensorflow outperformed the previous ~500,000 lines of code for\\nGoogle Translate’s \", _jsx(_components.em, {\n          children: \"extremely complicated\"\n        }), \" model. Blew my mind. \", _jsx(_components.sup, {\n          children: _jsx(_components.a, {\n            href: \"#user-content-fn-google\",\n            id: \"user-content-fnref-google\",\n            \"data-footnote-ref\": true,\n            \"aria-describedby\": \"footnote-label\",\n            children: \"1\"\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"As a software developer with a predominantly web-related skillset of Ruby,\\ndatabases, enough distributed systems knowledge to know to not get fancy, a bit\\nof hard-earned systems knowledge from debugging incidents, but only high school\\nlevel math: \", _jsx(_components.em, {\n          children: \"neural networks mystify me\"\n        }), \". How do they work? Why are they so\\ngood? Why are they so slow? Why are GPUs/TPUs used to speed them up? Why do the\\nbiggest models have more neurons than humans, yet still perform worse than the\\nhuman brain? \", _jsx(_components.sup, {\n          children: _jsx(_components.a, {\n            href: \"#user-content-fn-gpt3\",\n            id: \"user-content-fnref-gpt3\",\n            \"data-footnote-ref\": true,\n            \"aria-describedby\": \"footnote-label\",\n            children: \"2\"\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"In true napkin math fashion, the best course of action to answer those questions\\nis by implementing a simple neural net from scratch.\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        id: \"mental-model-for-a-neural-net-building-one-from-scratch\",\n        children: [\"Mental Model for a Neural Net: Building one from scratch\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#mental-model-for-a-neural-net-building-one-from-scratch\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The hardest part of napkin math isn’t the calculation itself: it’s acquiring the\\nconceptual understanding of a system to come up with an equation for its\\nperformance. Presenting and testing mental models of common systems is the crux\\nof value from the napkin math series!\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The simplest neural net we can draw might look something like this:\"\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/mental-model.jpg\",\n        alt: \"lol\",\n        width: \"687\",\n        height: \"598\",\n        priority: true,\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAALklEQVR4nGM4ceLEAzD4+fMng7i4OC8v76FDhzZs2MCwYcOG+fPnP3v27O3btwDdaRisnebS+wAAAABJRU5ErkJggg==\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Input layer\"\n          }), \". This is a representation of the data that we want to feed to\\nthe neural net. For example, the input layer for a 4x4 pixel grayscale image\\nthat looks like this \", _jsx(Rectangle, {\n            colors: [1, 1, 1, 0.2]\n          }), \" could be \", _jsx(_components.code, {\n            children: \"[1, 1, 1, 0.2]\"\n          }), \". Meaning the first 3 pixels are darkest (1.0) and the last pixel is\\nlighter (0.2).\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Hidden Layer\"\n          }), \". This is the layer that does a bunch of math on the input\\nlayer to convert it to our prediction. \", _jsx(_components.em, {\n            children: \"Training\"\n          }), \" a model refers to changing the\\nmath of the hidden layer(s) to more often create an output like the training\\ndata. We will go into more detail with this layer in a moment. The values in the\\nhidden layer are called \", _jsx(_components.em, {\n            children: \"weights\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [_jsx(_components.strong, {\n            children: \"Output Layer\"\n          }), \". This layer will contain our final prediction. For example,\\nif we feed it the rectangle from before \", _jsx(Rectangle, {\n            colors: [1, 1, 1, 0.2]\n          }), \" we\\nmight want the output layer to be a single number to represent how “dark” a\\nrectangle is, e.g.: \", _jsx(_components.code, {\n            children: \"0.8\"\n          }), \".\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"For example for the image \", _jsx(Rectangle, {\n          colors: [0.8, 0.7, 1, 1],\n          showInput: true\n        }), \" we’d expect a value close to 1 (dark!).\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"In contrast, for \", _jsx(Rectangle, {\n          colors: [0.2, 0.5, 0.4, 0.7],\n          showInput: true\n        }), \" we\\nexpect something closer to 0 than to 1.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Let’s implement a neural network from our simple mental model. The goal of this\\nneural network is to take a grayscale 2x2 image and tell us how “dark” it is\\nwhere 0 is completely white \", _jsx(Rectangle, {\n          colors: [0., 0., 0., 0.]\n        }), \", and 1 is\\ncompletely black \", _jsx(Rectangle, {\n          colors: [1., 1., 1., 1.]\n        }), \". We will initialize the\\nhidden layer with some random values at first, in Python:\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [\"input_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.5\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.7\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# We randomly initialize the weights (values) for the hidden layer... We will\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# need to \\\"train\\\" to make these weights give us the output layers we desire. We\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# will cover that shortly!\"\n          }), \"\\nhidden_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.98\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.86\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.08\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\\noutput_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This is really matrix multiplication. We explicitly _do not_ use a\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# matrix/tensor, because they add overhead to understanding what happens here\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# unless you work with them every day--which you probably don't. More on using\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# matrices later.\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"input_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"output_neuron\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 0.68\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Our neural network is giving us \", _jsx(Rectangle, {\n          colors: [0.2, 0.5, 0.4, 0.7],\n          result: 0.7\n        }), \" which is closer to ‘dark’ (1.0) than ‘light’ (0.0). When looking\\nat this rectangle as a human, we judge it to be more bright than dark, so we\\nwere expecting something below 0.5!\"]\n      }), \"\\n\", _jsx(Notice, {\n        children: _jsxs(_components.p, {\n          children: [\"There’s a \", _jsx(\"a\", {\n            href: \"https://colab.research.google.com/drive/1YRp9k_ORH4wZMqXLNkc3Ir5w4B5f-8Pa?usp=sharing\",\n            children: \"notebook\"\n          }), \" with the final code available. You can make a copy and execute it there. For early versions of the code, such as the above, you can create a new cell at the beginning of the notebook and build up from there!\"]\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"The only real thing we can change in our neural network in its current form is\\nthe hidden layer’s values. How do we change the hidden layer values so that the\\noutput neuron is close to 1 when the rectangle is dark, and close to 0 when it’s\\nlight?\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"We could abandon this approach and just take the average of all the pixels. That\\nwould work well! However, that’s not really the point of a neural net… We’ll\\nhit an impasse if we one day expand our model to try to implement\\n\", _jsx(_components.code, {\n          children: \"recognize_letters_from_picture(img)\"\n        }), \" or \", _jsx(_components.code, {\n          children: \"is_cat(img)\"\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Fundamentally, a neural network is just a way to approximate any function. It’s\\nreally hard to sit down and write \", _jsx(_components.code, {\n          children: \"is_cat\"\n        }), \", but the same technique we’re using\\nto implement \", _jsx(_components.code, {\n          children: \"average\"\n        }), \" through a neural network can be used to implement\\n\", _jsx(_components.code, {\n          children: \"is_cat\"\n        }), \". This is called the \", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/Universal_approximation_theorem\",\n          children: \"universal approximation theorem\"\n        }), \": an\\nartificial neural network can approximate \", _jsx(_components.em, {\n          children: \"any\"\n        }), \" function!\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"So, let’s try to teach our simple neural network to take the \", _jsx(_components.code, {\n          children: \"average()\"\n        }), \" of the\\npixels instead of explicitly telling it that that’s what we want! The idea of\\nthis walkthrough example is to understand a neural net with very few values and\\nlow complexity, otherwise it’s difficult to develop an intuition when we move to\\n1,000s of values and 10s of layers, as real neural networks have.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"We can observe that if we \", _jsx(_components.em, {\n          children: \"manually modify\"\n        }), \" all the hidden layer attributes to\\n\", _jsx(_components.code, {\n          children: \"0.25\"\n        }), \", our neural network is actually an average function!\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [\"input_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.5\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.7\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\nhidden_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.25\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.25\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.25\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.25\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\\noutput_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"input_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Two simple ways of calculating the same thing!\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"#\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# 0.2 * 0.25 + 0.5 * 0.25 + 0.4 * 0.25 + 0.7 * 25 = 0.45\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"output_neuron\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Here, we divide by 4 to get the average instead of\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# multiplying each element.\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"#\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# (0.2 + 0.5 + 0.4 + 0.7) / 4 = 0.45\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"sum\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"input_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"/\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(Rectangle, {\n          colors: [0.2, 0.5, 0.4, 0.7],\n          result: 0.45\n        }), \" sounds about right. The\\nrectangle is a little lighter than it’s dark.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"But that was cheating! We only showed that we \", _jsx(_components.em, {\n          children: \"can\"\n        }), \" implement \", _jsx(_components.code, {\n          children: \"average()\"\n        }), \" by\\nsimply changing the hidden layer’s values. But that won’t work if we try to implement\\nsomething more complicated. Let’s go back to our original hidden layer\\ninitialized with random values:\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [\"hidden_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.98\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.86\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.08\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"How can we \", _jsx(_components.em, {\n          children: \"teach\"\n        }), \" our neural network to implement \", _jsx(_components.code, {\n          children: \"average\"\n        }), \"?\"]\n      }), \"\\n\", _jsxs(_components.h2, {\n        id: \"training-our-neural-network\",\n        children: [\"Training our Neural Network\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#training-our-neural-network\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"To teach our model, we need to create some training data. We’ll create some\\nrectangles and calculate their average:\"\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [\"rectangles \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\nrectangle_average \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" i \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"range\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1000\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Generate a 2x2 rectangle [0.1, 0.8, 0.6, 1.0]\"\n          }), \"\\n    rectangle \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"round\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \"\\n                 \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"round\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \"\\n                 \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"round\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \"\\n                 \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"round\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n    rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"append\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Take the _actual_ average for our training dataset!\"\n          }), \"\\n    rectangle_average\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"append\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"sum\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"/\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Brilliant, so we can now feed these to our little neural network and get a\\nresult! Next step is for our neural network to adjust the values in the hidden\\nlayer based on how its output compares with the actual average in the training\\ndata. This is called our \", _jsx(_components.code, {\n          children: \"loss\"\n        }), \" function: large loss, very wrong model; small\\nloss, less wrong model. We can use a standard measure called \", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/Mean_squared_error\",\n          children: _jsx(_components.em, {\n            children: \"mean squared\\nerror\"\n          })\n        }), \":\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Take the average of all the differences squared!\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This calculates how \\\"wrong\\\" our predictions are.\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This is called our \\\"loss\\\".\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"mean_squared_error\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"actual\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" expected\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    error_sum \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" a\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" b \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"zip\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"actual\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" expected\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        error_sum \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"a \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-\"\n          }), \" b\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"**\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"2\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" error_sum \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"/\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"len\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"actual\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"mean_squared_error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1.\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"2.\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 1.0\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"mean_squared_error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1.\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"3.\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 4.0\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Now we can implement \", _jsx(_components.code, {\n          children: \"train()\"\n        }), \":\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"model\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" output_neuron\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"train\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n  outputs \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" rectangle \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n      output \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" model\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n      outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"append\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"output\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" outputs\\n\\nhidden_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.98\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.86\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.08\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\noutputs \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" train\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"10\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# [1.472, 0.7, 1.369, 0.8879, 1.392, 1.244, 0.644, 1.1179, 0.474, 1.54]\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle_average\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"10\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# [0.575, 0.45, 0.549, 0.35, 0.525, 0.475, 0.425, 0.65, 0.4, 0.575]\"\n          }), \"\\nmean_squared_error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" rectangle_average\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# 0.4218\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"A good mean squared error is close to 0. Our model isn’t very good. But! We’ve\\ngot the skeleton of a feedback loop in place for updating the hidden layer.\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        id: \"updating-the-hidden-layer-with-gradient-descent\",\n        children: [\"Updating the Hidden Layer with Gradient Descent\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#updating-the-hidden-layer-with-gradient-descent\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Now what we need is a way to update the hidden layer in response to the mean\\nsquared error / loss. We need to \", _jsx(_components.em, {\n          children: \"minimize\"\n        }), \" the value of this function:\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [\"mean_squared_error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"\\n  train\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \"\\n  rectangle_average\\n\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"As noted earlier, the only thing we can really change here are the weights in\\nthe hidden layer. How can we possibly know which weights will minimize this\\nfunction?\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We could randomize the weights, calculate the loss (how wrong the model is,\\nin our case, with mean squared error), and then save the best ones we see after\\nsome period of time.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We could possibly speed this up. If we have good weights, we could try to add\\nsome random numbers to those. See if loss improves. This could work, but it\\nsounds slow… and likely to get stuck in some local maxima and not give a very\\ngood result. And it’s trouble scaling this to 1,000s of weights…\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Instead of embarking on this ad-hoc randomization mess, it turns out that\\nthere’s a method called \", _jsx(_components.em, {\n          children: \"gradient descent\"\n        }), \" to minimize the value of a function!\\nGradient descent builds on a bit of calculus that you may not have touched on\\nsince high school. We won’t go into depth here, but try to introduce \", _jsx(_components.em, {\n          children: \"just\"\n        }), \"\\nenough that you understand the concept. \", _jsx(_components.sup, {\n          children: _jsx(_components.a, {\n            href: \"#user-content-fn-3blue1brown\",\n            id: \"user-content-fnref-3blue1brown\",\n            \"data-footnote-ref\": true,\n            \"aria-describedby\": \"footnote-label\",\n            children: \"3\"\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Let’s try to understand gradient descent. Consider some random function whose\\ngraph might look like this:\"\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/function.png\",\n        alt: \"Graph of a random function with some irregular shapes\",\n        title: \"Graph of a function with an irregular curve with a local and global minimum.\",\n        width: \"775\",\n        height: \"485\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFtaAABbWgFGH/T3AAAAL0lEQVR4nGNYtGgRAxgcevmfYdPX/8d+/z/w9v+Sl/8ZFi3cOn/v3i337i7YcgoAlZ4XzUe5RowAAAAASUVORK5CYII=\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"How do we write code to find the minimum, the deepest (second) valley, of this function?\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Let’s say that we’re at \", _jsx(_components.code, {\n          children: \"x=1\"\n        }), \" and we know the \", _jsx(_components.em, {\n          children: \"slope\"\n        }), \" of the function at this\\npoint. The slope is “how fast the function grows at this very point.” You may\\nremember this as \", _jsx(_components.em, {\n          children: \"the derivative\"\n        }), \". The slope at \", _jsx(_components.code, {\n          children: \"x=1\"\n        }), \" might be \", _jsx(_components.code, {\n          children: \"-1.5\"\n        }), \". This\\nmeans that every time we increase \", _jsx(_components.code, {\n          children: \"x += 1\"\n        }), \", it results in \", _jsx(_components.code, {\n          children: \"y -= 1.5\"\n        }), \". We’ll go\\ninto how you figure out the slope in a bit, let’s focus on the concept first.\"]\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/function-with-slope.png\",\n        alt: \"Graph function with some slope or derivative\",\n        width: \"1934\",\n        height: \"1256\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFtaAABbWgFGH/T3AAAAL0lEQVR4nGOYNm0aAxhsuPGf4datZ6fv/j/0+//iN/8ZLmw68PDs2YXHb89Yvx4AkEUXyxb02LEAAAAASUVORK5CYII=\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The idea of gradient descent is that since we know the value of our function,\\n\", _jsx(_components.code, {\n          children: \"y\"\n        }), \", is decreasing as we increase \", _jsx(_components.code, {\n          children: \"x\"\n        }), \", we can increase \", _jsx(_components.code, {\n          children: \"x\"\n        }), \" proportionally to the\\nslope. In other words, if we increase \", _jsx(_components.code, {\n          children: \"x\"\n        }), \" by the slope, we step towards the\\nvalley by \", _jsx(_components.code, {\n          children: \"1.5\"\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Let’s take that step of \", _jsx(_components.code, {\n          children: \"x += 1.5\"\n        }), \":\"]\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/gradient-descent-overshoot.png\",\n        alt: \"Overshooting in gradient descent\",\n        width: \"1934\",\n        height: \"1256\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFtaAABbWgFGH/T3AAAALklEQVR4nGPI6OlmAIOVl/4znLj68P+n/4ce/1/18z/D8VU7bhw50rXvSv/SpQCFDxcsf747hwAAAABJRU5ErkJggg==\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Ugh, turned out that we stepped \", _jsx(_components.em, {\n          children: \"too\"\n        }), \" far, past this valley! If we repeat the\\nstep, we’ll land somewhere on the left side of the valley, to then bounce back\\non the right side. We might \", _jsx(_components.em, {\n          children: \"never\"\n        }), \" land in the bottom of the valley. Bummer.\\nEither way, this isn’t the \", _jsx(_components.em, {\n          children: \"global minimum\"\n        }), \" of the function. We return to that\\nin a moment!\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"We can fix the overstepping easily by taking smaller steps. Perhaps we should’ve\\nstepped by just \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mn, {\n                      children: \"0.1\"\n                    }), _jsx(_components.mo, {\n                      children: \"∗\"\n                    }), _jsx(_components.mn, {\n                      children: \"1.5\"\n                    }), _jsx(_components.mo, {\n                      children: \"=\"\n                    }), _jsx(_components.mn, {\n                      children: \"0.15\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"0.1 * 1.5 = 0.15\"\n                  })]\n                })\n              })\n            }), _jsxs(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: [_jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"0.1\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2222em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mbin\",\n                  children: \"∗\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2222em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"1.5\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mrel\",\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"0.15\"\n                })]\n              })]\n            })]\n          })\n        }), \" instead. That would’ve smoothly landed us at\\nthe bottom of the valley. That multiplier, \", _jsx(_components.code, {\n          children: \"0.1\"\n        }), \", is called the \", _jsx(_components.em, {\n          children: \"learning rate\"\n        }), \"\\nin gradient descent.\"]\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/minimum.png\",\n        alt: \"Minimum of function with gradient descent\",\n        width: \"1934\",\n        height: \"1256\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAFteAABbXgG54nPlAAAAL0lEQVR4nGNgYWHp+n8q9+gRBp80Bj1LF9XSBQx1ExkyOxh4eaX4VVQYRFQYxMUB74YJ3gaVuHkAAAAASUVORK5CYII=\"\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"But hang on, that’s not actually the minimum of the function. See that valley to\\nthe right? That’s the \", _jsx(_components.em, {\n          children: \"actual\"\n        }), \" global minimum. If our initial \", _jsx(_components.code, {\n          children: \"x\"\n        }), \" value had been\\ne.g. 3, we might have found the global minimum instead of our local minimum.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Finding the global minimum of a function is \", _jsx(_components.em, {\n          children: \"hard\"\n        }), \". Gradient descent will give\\nus \", _jsx(_components.em, {\n          children: \"a minimum\"\n        }), \", but not \", _jsx(_components.em, {\n          children: \"the minimum\"\n        }), \". Unfortunately, it turns out it’s the best\\nweapon we have at our disposal. Especially when we have big, complicated\\nfunctions (like a neural net with millions of neurons). Gradient descent will\\nnot always find the global minimum, but something \", _jsx(_components.em, {\n          children: \"pretty\"\n        }), \" good.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"This method of using the slope/derivative generalizes. For example, consider\\noptimizing a function in three-dimensions. We can visualize the gradient descent\\nmethod here as \", _jsx(_components.em, {\n          children: \"rolling a ball to the lowest point.\"\n        }), \" A big neural network is\\n1000s of dimensions, but gradient descent still works to minimize the loss!\"]\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/descent-3d.png\",\n        alt: \"\",\n        title: \"Depicts a 3-dimensional graph, if we do gradient descent on this we might imagine it as rolling a ball down the hill.\",\n        width: \"760\",\n        height: \"624\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAMklEQVR4nAEnANj/APb29o1WeWxUd7u6ywDIz7YYLQkACCSBgpsA6vDlr9Oq+vv7+/z7mscW6F9NiW0AAAAASUVORK5CYII=\"\n      }), \"\\n\", _jsxs(_components.h2, {\n        id: \"finalizing-our-neural-network-from-scratch\",\n        children: [\"Finalizing our Neural Network from scratch\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#finalizing-our-neural-network-from-scratch\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Let’s summarize where we are:\"\n      }), \"\\n\", _jsxs(_components.ul, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"We can implement a simple neural net: \", _jsx(_components.code, {\n            children: \"model()\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Our neural net can figure out how \", _jsx(_components.em, {\n            children: \"wrong\"\n          }), \" it is for a training set: \", _jsx(_components.code, {\n            children: \"loss(train())\"\n          }), \".\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"We have a method, \", _jsx(_components.em, {\n            children: \"gradient descent\"\n          }), \", for tuning our hidden layer’s weights\\nfor the minimum loss. I.e. we have a method to adjust those four random values\\nin our hidden layer to take a \", _jsx(_components.em, {\n            children: \"better\"\n          }), \" average as we iterate through the\\ntraining data.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Now, let’s implement gradient descent and see if we can make our neural net\\nlearn to take the average grayscale of our small rectangles:\"\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"model\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" output_neuron\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"train\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n  outputs \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" rectangle \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n      output \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" model\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n      outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"append\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"output\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n  mean_squared_error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" rectangle_average\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# We go through all the weights in the hidden layer. These correspond to all\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# the weights of the function we're trying to minimize the value of: our\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# model, respective of its loss (how wrong it is).\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# \"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# For each of the weights, we want to increase/decrease it based on the slope.\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Exactly like we showed in the one-weight example above with just x. Now\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# we just have 4 values instead of 1! Big models have billions.\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" _ \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    learning_rate \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.1\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# But... how do we get the slope/derivative?!\"\n          }), \"\\n    hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-=\"\n          }), \" learning_rate \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"slope\\n\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" outputs\\n\\nhidden_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.98\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.86\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.08\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\ntrain\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.h3, {\n        id: \"automagically-computing-the-slope-of-a-function-with-autograd\",\n        children: [\"Automagically computing the slope of a function with \", _jsx(_components.code, {\n          children: \"autograd\"\n        }), _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#automagically-computing-the-slope-of-a-function-with-autograd\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The missing piece here is to figure out the \", _jsx(_components.code, {\n          children: \"slope()\"\n        }), \" after we’ve gone through\\nour training set. Figuring out the slope/derivative at a certain point is\\ntricky. It involves a fair bit of math. I am not going to go into the math of\\ncalculating derivatives. Instead, we’ll do what all the machine learning\\nlibraries do: automatically calculate it. \", _jsx(_components.sup, {\n          children: _jsx(_components.a, {\n            href: \"#user-content-fn-nielsen\",\n            id: \"user-content-fnref-nielsen\",\n            \"data-footnote-ref\": true,\n            \"aria-describedby\": \"footnote-label\",\n            children: \"4\"\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Minimizing the loss of a function is absolutely fundamental to machine learning.\\nThe functions (neural networks) are \", _jsx(_components.em, {\n          children: \"so\"\n        }), \" complicated that manually sitting down\\nto figure out the derivative like you might’ve done in high school is not\\nfeasible. It’s the mathematical equivalent of writing assembly to implement a\\nwebsite.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Let’s show one simple example of finding the derivative of a function, before we\\nlet the computers do it all for us. If we have \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mi, {\n                      children: \"f\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    }), _jsx(_components.mo, {\n                      children: \"=\"\n                    }), _jsxs(_components.msup, {\n                      children: [_jsx(_components.mi, {\n                        children: \"x\"\n                      }), _jsx(_components.mn, {\n                        children: \"2\"\n                      })]\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"f(x) = x^2\"\n                  })]\n                })\n              })\n            }), _jsxs(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: [_jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.10764em\"\n                  },\n                  children: \"f\"\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mrel\",\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.8141em\"\n                  }\n                }), _jsxs(_components.span, {\n                  className: \"mord\",\n                  children: [_jsx(_components.span, {\n                    className: \"mord mathnormal\",\n                    children: \"x\"\n                  }), _jsx(_components.span, {\n                    className: \"msupsub\",\n                    children: _jsx(_components.span, {\n                      className: \"vlist-t\",\n                      children: _jsx(_components.span, {\n                        className: \"vlist-r\",\n                        children: _jsx(_components.span, {\n                          className: \"vlist\",\n                          style: {\n                            height: \"0.8141em\"\n                          },\n                          children: _jsxs(_components.span, {\n                            style: {\n                              top: \"-3.063em\",\n                              marginRight: \"0.05em\"\n                            },\n                            children: [_jsx(_components.span, {\n                              className: \"pstrut\",\n                              style: {\n                                height: \"2.7em\"\n                              }\n                            }), _jsx(_components.span, {\n                              className: \"sizing reset-size6 size3 mtight\",\n                              children: _jsx(_components.span, {\n                                className: \"mord mtight\",\n                                children: \"2\"\n                              })\n                            })]\n                          })\n                        })\n                      })\n                    })\n                  })]\n                })]\n              })]\n            })]\n          })\n        }), \", then you might\\nremember from calculus classes that the derivative is \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsxs(_components.msup, {\n                      children: [_jsx(_components.mi, {\n                        children: \"f\"\n                      }), _jsx(_components.mo, {\n                        mathvariant: \"normal\",\n                        lspace: \"0em\",\n                        rspace: \"0em\",\n                        children: \"′\"\n                      })]\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    }), _jsx(_components.mo, {\n                      children: \"=\"\n                    }), _jsx(_components.mn, {\n                      children: \"2\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"f'(x) = 2x\"\n                  })]\n                })\n              })\n            }), _jsxs(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: [_jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1.0019em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsxs(_components.span, {\n                  className: \"mord\",\n                  children: [_jsx(_components.span, {\n                    className: \"mord mathnormal\",\n                    style: {\n                      marginRight: \"0.10764em\"\n                    },\n                    children: \"f\"\n                  }), _jsx(_components.span, {\n                    className: \"msupsub\",\n                    children: _jsx(_components.span, {\n                      className: \"vlist-t\",\n                      children: _jsx(_components.span, {\n                        className: \"vlist-r\",\n                        children: _jsx(_components.span, {\n                          className: \"vlist\",\n                          style: {\n                            height: \"0.7519em\"\n                          },\n                          children: _jsxs(_components.span, {\n                            style: {\n                              top: \"-3.063em\",\n                              marginRight: \"0.05em\"\n                            },\n                            children: [_jsx(_components.span, {\n                              className: \"pstrut\",\n                              style: {\n                                height: \"2.7em\"\n                              }\n                            }), _jsx(_components.span, {\n                              className: \"sizing reset-size6 size3 mtight\",\n                              children: _jsx(_components.span, {\n                                className: \"mord mtight\",\n                                children: _jsx(_components.span, {\n                                  className: \"mord mtight\",\n                                  children: \"′\"\n                                })\n                              })\n                            })]\n                          })\n                        })\n                      })\n                    })\n                  })]\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mrel\",\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"2\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                })]\n              })]\n            })]\n          })\n        }), \". In other\\nwords, \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mi, {\n                      children: \"f\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"f(x)\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.10764em\"\n                  },\n                  children: \"f\"\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                })]\n              })\n            })]\n          })\n        }), \"’s slope at any point is \", _jsx(_components.code, {\n          children: \"2x\"\n        }), \", telling us it’s increasing\\nnon-linearly. Well that’s exactly how we understand \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsx(_components.mrow, {\n                    children: _jsxs(_components.msup, {\n                      children: [_jsx(_components.mi, {\n                        children: \"x\"\n                      }), _jsx(_components.mn, {\n                        children: \"2\"\n                      })]\n                    })\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"x^2\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.8141em\"\n                  }\n                }), _jsxs(_components.span, {\n                  className: \"mord\",\n                  children: [_jsx(_components.span, {\n                    className: \"mord mathnormal\",\n                    children: \"x\"\n                  }), _jsx(_components.span, {\n                    className: \"msupsub\",\n                    children: _jsx(_components.span, {\n                      className: \"vlist-t\",\n                      children: _jsx(_components.span, {\n                        className: \"vlist-r\",\n                        children: _jsx(_components.span, {\n                          className: \"vlist\",\n                          style: {\n                            height: \"0.8141em\"\n                          },\n                          children: _jsxs(_components.span, {\n                            style: {\n                              top: \"-3.063em\",\n                              marginRight: \"0.05em\"\n                            },\n                            children: [_jsx(_components.span, {\n                              className: \"pstrut\",\n                              style: {\n                                height: \"2.7em\"\n                              }\n                            }), _jsx(_components.span, {\n                              className: \"sizing reset-size6 size3 mtight\",\n                              children: _jsx(_components.span, {\n                                className: \"mord mtight\",\n                                children: \"2\"\n                              })\n                            })]\n                          })\n                        })\n                      })\n                    })\n                  })]\n                })]\n              })\n            })]\n          })\n        }), \", perfect! This means\\nthat for \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      children: \"=\"\n                    }), _jsx(_components.mn, {\n                      children: \"2\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"x = 2\"\n                  })]\n                })\n              })\n            }), _jsxs(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: [_jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.4306em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mrel\",\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"2\"\n                })]\n              })]\n            })]\n          })\n        }), \" the slope is \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsx(_components.mrow, {\n                    children: _jsx(_components.mn, {\n                      children: \"4\"\n                    })\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"4\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"4\"\n                })]\n              })\n            })]\n          })\n        }), \".\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"With the basics in order, we can use an \", _jsx(_components.code, {\n          children: \"autograd\"\n        }), \" package to avoid the messy\\nbusiness of computing our own derivatives. \", _jsx(_components.code, {\n          children: \"autograd\"\n        }), \" is an \", _jsx(_components.em, {\n          children: \"automatic\\ndifferentiation engine\"\n        }), \". \", _jsx(_components.em, {\n          children: \"grad\"\n        }), \" stands for \", _jsx(_components.em, {\n          children: \"gradient\"\n        }), \", which we can think of as the\\nderivative/slope of a function with more than one parameter.\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"It’s best to show how it works by using our example from before:\"\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"import\"\n          }), \" torch\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# A tensor is a matrix in PyTorch. It is the fundamental data-structure of neural\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# networks. Here we say PyTorch, please keep track of the gradient/derivative\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# as I do all kinds of things to the parameter(s) of this tensor.\"\n          }), \"\\nx \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" torch\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"tensor\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"2.\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" requires_grad\", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), _jsx(_components.span, {\n            className: \"token boolean\",\n            children: \"True\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# At this point we're applying our function f(x) = x^2.\"\n          }), \"\\ny \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" x \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"**\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"2\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This tells `autograd` to compute the derivative values for all the parameters\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# involved. Backward is neural network jargon for this operation, which we'll\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# explain momentarily.\"\n          }), \"\\ny\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"backward\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# And show us the lovely gradient/derivative, which is 4! Sick.\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"x\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"grad\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 4\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(_components.code, {\n          children: \"autograd\"\n        }), \" is the closest to magic we get. I could do the most ridiculous stuff\\nwith this tensor, and it’ll keep track of all the math operations applied and\\nhave the ability to compute the derivative. We won’t go into how. Partly because\\nI don’t know how, and this post is long enough.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Just to convince you of this, we can be a little cheeky and do a bunch of random\\nstuff. I’m trying to really hammer this home, because this is what confused me\\nthe most when learning about neural networks. It wasn’t obvious to me that a\\nneural network, including executing the loss function on the whole training set,\\nis \", _jsx(_components.em, {\n          children: \"just\"\n        }), \" a function, and however complicated, we can still take the derivative\\nof it and use gradient descent. Even if it’s so many dimensions that it can’t be\\nneatly visualized as a ball rolling down a hill.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(_components.code, {\n          children: \"autograd\"\n        }), \" doesn’t complain as we add complexity and will still calculate the\\ngradients. In this example we’ll even use a matrix/tensor with a few more elements and\\ncalculate an average (like our loss function \", _jsx(_components.code, {\n          children: \"mean_squared_error\"\n        }), \"), which is the\\nkind of thing we’ll calculate the gradients for in our neural network:\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"import\"\n          }), \" random\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"import\"\n          }), \" torch\\n\\nx \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" torch\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"tensor\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.3\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.8\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" requires_grad\", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), _jsx(_components.span, {\n            className: \"token boolean\",\n            children: \"True\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\ny \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" x\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" _ \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"range\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"3\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    choice \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"randint\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"if\"\n          }), \" choice \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"==\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        y \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" y \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"**\"\n          }), \" random\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"randint\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"10\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"elif\"\n          }), \" choice \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"==\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"1\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        y \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" y\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"sqrt\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"elif\"\n          }), \" choice \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"==\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        y \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" y\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"atanh\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\ny \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" y\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"mean\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This walks \\\"backwards\\\" y all the way to the parameters to\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# calculate the derivates / gradient! Pytorch keeps track of a graph of all the\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# operations.\"\n          }), \"\\ny\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"backward\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# And here are how quickly the function is changing with respect to these\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# parameters for our randomized function.\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"x\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"grad\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e tensor([0.0157, 0.0431, 0.6338, 0.0028])\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Let’s use \", _jsx(_components.code, {\n          children: \"autograd\"\n        }), \" for our neural net and then run it against our square from\\nearlier \", _jsx(Rectangle, {\n          colors: [0.2, 0.5, 0.4, 0.7],\n          result: 0.45\n        }), \":\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"import\"\n          }), \" torch \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"as\"\n          }), \" torch\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"model\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" output_neuron\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"train\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n  outputs \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" rectangle \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n      output \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" model\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n      outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"append\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"output\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# How wrong were we? Our 'loss.'\"\n          }), \"\\n  error \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" mean_squared_error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"outputs\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" rectangle_average\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Calculate the gradient (the derivate for all our weights!)\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This walks \\\"backwards\\\" from the error all the way to the weights to\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# calculate them\"\n          }), \"\\n  error\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"backward\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Now let's go update the weights in our hidden layer per our gradient.\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# This is what we discussed before: we want to find the valley of this\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# four-dimensional space/four-weight function. This is gradient descent!\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" _ \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    learning_rate \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.1\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# hidden_layer.grad is something like [0.7070, 0.6009, 0.6840, 0.5302]\"\n          }), \"\\n    hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"data\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-=\"\n          }), \" learning_rate \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"grad\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"data\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# We have to tell `autograd` that we've just finished an epoch to reset.\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Otherwise it'd calculate the derivative from multiple epochs.\"\n          }), \"\\n  hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"grad\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"zero_\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n  \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" error\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# We use tensors now, but we just use them as if they were normal lists.\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# We only use them so we can get the gradients.\"\n          }), \"\\nhidden_layer \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" torch\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"tensor\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.98\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.86\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"-\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.08\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" requires_grad\", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), _jsx(_components.span, {\n            className: \"token boolean\",\n            children: \"True\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"model\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.5\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.7\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 0.6840000152587891\"\n          }), \"\\n\\ntrain\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# The hidden layer's weights are nudging closer to [0.25, 0.25, 0.25, 0.25]!\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# They are now [ 0.9093,  0.3399,  0.7916, -0.1330]\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsxs(_components.span, {\n            className: \"token string-interpolation\",\n            children: [_jsx(_components.span, {\n              className: \"token string\",\n              children: \"f\\\"After: \"\n            }), _jsxs(_components.span, {\n              className: \"token interpolation\",\n              children: [_jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"{\"\n              }), \"model\", _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"(\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"[\"\n              }), _jsx(_components.span, {\n                className: \"token number\",\n                children: \"0.2\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \",\"\n              }), _jsx(_components.span, {\n                className: \"token number\",\n                children: \"0.5\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \",\"\n              }), _jsx(_components.span, {\n                className: \"token number\",\n                children: \"0.4\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \",\"\n              }), _jsx(_components.span, {\n                className: \"token number\",\n                children: \"0.7\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"]\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \",\"\n              }), \" hidden_layer\", _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \")\"\n              }), _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"}\"\n              })]\n            }), _jsx(_components.span, {\n              className: \"token string\",\n              children: \"\\\"\"\n            })]\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 0.5753424167633057\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# The average of this rectangle is 0.45, closer... but not there yet\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"This blew my mind the first time I did this. Look at that. It’s optimizing the\\nhidden layer for all weights in the right direction! We’re expecting them all\\nto nudge towards \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsx(_components.mrow, {\n                    children: _jsx(_components.mn, {\n                      children: \"0.25\"\n                    })\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"0.25\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"0.25\"\n                })]\n              })\n            })]\n          })\n        }), \" to implement \", _jsx(_components.code, {\n          children: \"average()\"\n        }), \". We haven’t told it \", _jsx(_components.em, {\n          children: \"anything\"\n        }), \"\\nabout average, we’ve just told it how wrong it is through the loss.\"]\n      }), \"\\n\", _jsx(Notice, {\n        children: _jsxs(_components.p, {\n          children: [\"It’s important to understand how \", _jsx(\"code\", {\n            children: \"hidden_layer.grad\"\n          }), \" is set here. The\\nhidden layer is instantiated as a tensor with an argument telling Pytorch to\\nkeep track of all operations made to it. This allows us to later call \", _jsx(\"code\", {\n            children: \"backward()\"\n          }), \" on a future tensor that derives from the hidden layer,\\nin this case, the \", _jsx(\"code\", {\n            children: \"error\"\n          }), \" tensor, which is further derived from the\\n\", _jsx(\"code\", {\n            children: \" outputs\"\n          }), \" tensor. You can read more in \", _jsx(\"a\", {\n            href: \"https://pytorch.org/docs/1.9.1/generated/torch.Tensor.backward.html\",\n            children: \"the documentation\"\n          })]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [_jsx(_components.em, {\n          children: \"But\"\n        }), \", the hidden layer isn’t all \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsx(_components.mrow, {\n                    children: _jsx(_components.mn, {\n                      children: \"0.25\"\n                    })\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"0.25\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"0.25\"\n                })]\n              })\n            })]\n          })\n        }), \" quite yet, as we expect for it to\\nimplement \", _jsx(_components.code, {\n          children: \"average\"\n        }), \". So how do we get them to that? Well, let’s try to repeat\\nthe gradient descent process 100 times and see if we’re getting even better!\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# An epoch is a training pass over the full data set!\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" epoch \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"range\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"100\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n   error \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" train\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangles\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n   \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsxs(_components.span, {\n            className: \"token string-interpolation\",\n            children: [_jsx(_components.span, {\n              className: \"token string\",\n              children: \"f\\\"Epoch: \"\n            }), _jsxs(_components.span, {\n              className: \"token interpolation\",\n              children: [_jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"{\"\n              }), \"epoch\", _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"}\"\n              })]\n            }), _jsx(_components.span, {\n              className: \"token string\",\n              children: \", Error: \"\n            }), _jsxs(_components.span, {\n              className: \"token interpolation\",\n              children: [_jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"{\"\n              }), \"error\", _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"}\"\n              })]\n            }), _jsx(_components.span, {\n              className: \"token string\",\n              children: \", Layer: \"\n            }), _jsxs(_components.span, {\n              className: \"token interpolation\",\n              children: [_jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"{\"\n              }), \"hidden_layer\", _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \".\"\n              }), \"data\", _jsx(_components.span, {\n                className: \"token punctuation\",\n                children: \"}\"\n              })]\n            }), _jsx(_components.span, {\n              className: \"token string\",\n              children: \"\\\\n\\\\n\\\"\"\n            })]\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n   \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# \"\n          }), \"\\n   \", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"#  Epoch: 99, Error: 0.0019292341312393546, Layer: tensor([0.3251, 0.2291, 0.3075, 0.1395])\"\n          }), \"\\n\\n\\n\", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"model\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.5\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.7\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"item\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# =\u003e 0.4002\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Pretty close, but not quite there. I ran it for \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsx(_components.mrow, {\n                    children: _jsx(_components.mn, {\n                      children: \"300\"\n                    })\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"300\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"300\"\n                })]\n              })\n            })]\n          })\n        }), \" times (an iteration over\\nthe full training set is referred to as an epoch, so 300 epochs) instead, and\\nthen I got:\"]\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"print\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"model\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.2\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.5\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.4\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.7\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \".\"\n          }), \"item\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# Epoch: 299, Error: 1.8315197394258576e-06, Layer: tensor([0.2522, 0.2496, 0.2518, 0.2465])\"\n          }), \"\\n\", _jsx(_components.span, {\n            className: \"token comment\",\n            children: \"# tensor(0.4485, grad_fn=\u003cAddBackward0\u003e)\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Boom! Our neural net has \", _jsx(_components.em, {\n          children: \"almost\"\n        }), \" learned to take the average, off by just a\\nscanty \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsx(_components.mrow, {\n                    children: _jsx(_components.mn, {\n                      children: \"0.002\"\n                    })\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"0.002\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.6444em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"0.002\"\n                })]\n              })\n            })]\n          })\n        }), \". If we fine-tuned the learning rate and number of epochs we could\\nprobably get it there, but I’m happy with this. \", _jsx(Rectangle, {\n          colors: [0.2, 0.5, 0.4, 0.7],\n          result: 0.448\n        }), \":\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"That’s it. That’s your first neural net:\"\n      }), \"\\n\", _jsx(_components.div, {\n        className: \"math math-display\",\n        children: _jsx(_components.span, {\n          className: \"katex-display\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                display: \"block\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mi, {\n                      children: \"m\"\n                    }), _jsx(_components.mi, {\n                      children: \"o\"\n                    }), _jsx(_components.mi, {\n                      children: \"d\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mi, {\n                      children: \"l\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mi, {\n                      children: \"r\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mi, {\n                      children: \"c\"\n                    }), _jsx(_components.mi, {\n                      children: \"t\"\n                    }), _jsx(_components.mi, {\n                      children: \"a\"\n                    }), _jsx(_components.mi, {\n                      children: \"n\"\n                    }), _jsx(_components.mi, {\n                      children: \"g\"\n                    }), _jsx(_components.mi, {\n                      children: \"l\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    }), _jsx(_components.mo, {\n                      children: \"≈\"\n                    }), _jsx(_components.mi, {\n                      children: \"a\"\n                    }), _jsx(_components.mi, {\n                      children: \"v\"\n                    }), _jsx(_components.mi, {\n                      children: \"g\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mi, {\n                      children: \"r\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mi, {\n                      children: \"c\"\n                    }), _jsx(_components.mi, {\n                      children: \"t\"\n                    }), _jsx(_components.mi, {\n                      children: \"a\"\n                    }), _jsx(_components.mi, {\n                      children: \"n\"\n                    }), _jsx(_components.mi, {\n                      children: \"g\"\n                    }), _jsx(_components.mi, {\n                      children: \"l\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"model(rectangle) \\\\approx avg(rectangle)\"\n                  })]\n                })\n              })\n            }), _jsxs(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: [_jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"m\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"o\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"d\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"e\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.01968em\"\n                  },\n                  children: \"l\"\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"rec\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"t\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"an\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.03588em\"\n                  },\n                  children: \"g\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.01968em\"\n                  },\n                  children: \"l\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"e\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mrel\",\n                  children: \"≈\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"a\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.03588em\"\n                  },\n                  children: \"vg\"\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"rec\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"t\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"an\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.03588em\"\n                  },\n                  children: \"g\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.01968em\"\n                  },\n                  children: \"l\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"e\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                })]\n              })]\n            })]\n          })\n        })\n      }), \"\\n\", _jsxs(_components.h2, {\n        id: \"ok-so-you-just-implemented-the-most-complicated-average-function-ive-ever-seen\",\n        children: [\"OK, so you just implemented the most complicated \", _jsx(_components.code, {\n          children: \"average\"\n        }), \" function I’ve ever seen…\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#ok-so-you-just-implemented-the-most-complicated-average-function-ive-ever-seen\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Sure did. The thing is, that if we adjusted it for looking for cats, it’s the\\nleast complicated \", _jsx(_components.code, {\n          children: \"is_cat\"\n        }), \" you’ll ever see. Because our neural network could\\nimplement that too by changing the training data. Remember, a neural network\\nwith enough neurons can approximate \", _jsx(_components.em, {\n          children: \"any function\"\n        }), \". You’ve just learned all the\\nbuilding blocks to do it. We just started with the simplest possible example.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"If you give the hidden layer some more neurons, this neural net will be able to\\nrecognize \", _jsx(_components.a, {\n          href: \"http://yann.lecun.com/exdb/mnist/\",\n          children: \"handwritten numbers\"\n        }), \" with decent accuracy (possible fun\\nexercise for you, see bottom of article), like this one:\"]\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/mnist-sample.png\",\n        alt: \"\",\n        title: \"An upscaled version of a handdrawn 3 from the 28x28 MNIST dataset.\",\n        width: \"200\",\n        height: \"200\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAAECAIAAAAmkwkpAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAMklEQVR4nGNgYGDw8PAICQlhgICenp7///+DWMzMzCUlJbt37wZx2NjYKisrLS0tgRwAO/EMNDi6S+4AAAAASUVORK5CYII=\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        id: \"activation-functions\",\n        children: [\"Activation Functions\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#activation-functions\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"To be truly powerful, there is one paramount modification we have to make to our\\nneural net. Above, we were implementing the \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mi, {\n                      children: \"a\"\n                    }), _jsx(_components.mi, {\n                      children: \"v\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mi, {\n                      children: \"r\"\n                    }), _jsx(_components.mi, {\n                      children: \"a\"\n                    }), _jsx(_components.mi, {\n                      children: \"g\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"average\"\n                  })]\n                })\n              })\n            }), _jsx(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"0.625em\",\n                    verticalAlign: \"-0.1944em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"a\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.03588em\"\n                  },\n                  children: \"v\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.02778em\"\n                  },\n                  children: \"er\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"a\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.03588em\"\n                  },\n                  children: \"g\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"e\"\n                })]\n              })\n            })]\n          })\n        }), \" function. However, were\\nour neural net to implement \", _jsx(_components.code, {\n          children: \"which_digit(png)\"\n        }), \" or \", _jsx(_components.code, {\n          children: \"is_cat(jpg)\"\n        }), \" then it wouldn’t work.\"]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"Recognizing handwritten digits isn’t a \", _jsx(_components.em, {\n          children: \"linear\"\n        }), \" function, like \", _jsx(_components.code, {\n          children: \"average()\"\n        }), \". It’s\\nnon-linear. It’s a crazy function, with a crazy shape (unlike a linear\\nfunction). To create crazy functions with crazy shapes, we have to introduce a\\nnon-linear component to our neural network. This is called an \", _jsx(_components.em, {\n          children: \"activation\"\n        }), \"\\nfunction. It can be e.g. \", _jsx(_components.span, {\n          className: \"math math-inline\",\n          children: _jsxs(_components.span, {\n            className: \"katex\",\n            children: [_jsx(_components.span, {\n              className: \"katex-mathml\",\n              children: _jsx(_components.math, {\n                xmlns: \"http://www.w3.org/1998/Math/MathML\",\n                children: _jsxs(_components.semantics, {\n                  children: [_jsxs(_components.mrow, {\n                    children: [_jsx(_components.mi, {\n                      children: \"R\"\n                    }), _jsx(_components.mi, {\n                      children: \"e\"\n                    }), _jsx(_components.mi, {\n                      children: \"L\"\n                    }), _jsx(_components.mi, {\n                      children: \"u\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    }), _jsx(_components.mo, {\n                      children: \"=\"\n                    }), _jsx(_components.mi, {\n                      children: \"m\"\n                    }), _jsx(_components.mi, {\n                      children: \"a\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \"(\"\n                    }), _jsx(_components.mn, {\n                      children: \"0\"\n                    }), _jsx(_components.mo, {\n                      separator: \"true\",\n                      children: \",\"\n                    }), _jsx(_components.mi, {\n                      children: \"x\"\n                    }), _jsx(_components.mo, {\n                      stretchy: \"false\",\n                      children: \")\"\n                    })]\n                  }), _jsx(_components.annotation, {\n                    encoding: \"application/x-tex\",\n                    children: \"ReLu(x) = max(0, x)\"\n                  })]\n                })\n              })\n            }), _jsxs(_components.span, {\n              className: \"katex-html\",\n              \"aria-hidden\": \"true\",\n              children: [_jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  style: {\n                    marginRight: \"0.00773em\"\n                  },\n                  children: \"R\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"e\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"Lu\"\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mrel\",\n                  children: \"=\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.2778em\"\n                  }\n                })]\n              }), _jsxs(_components.span, {\n                className: \"base\",\n                children: [_jsx(_components.span, {\n                  className: \"strut\",\n                  style: {\n                    height: \"1em\",\n                    verticalAlign: \"-0.25em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"ma\"\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mopen\",\n                  children: \"(\"\n                }), _jsx(_components.span, {\n                  className: \"mord\",\n                  children: \"0\"\n                }), _jsx(_components.span, {\n                  className: \"mpunct\",\n                  children: \",\"\n                }), _jsx(_components.span, {\n                  className: \"mspace\",\n                  style: {\n                    marginRight: \"0.1667em\"\n                  }\n                }), _jsx(_components.span, {\n                  className: \"mord mathnormal\",\n                  children: \"x\"\n                }), _jsx(_components.span, {\n                  className: \"mclose\",\n                  children: \")\"\n                })]\n              })]\n            })]\n          })\n        }), \". There are many kinds of\\n\", _jsx(_components.a, {\n          href: \"https://en.wikipedia.org/wiki/Activation_function\",\n          children: \"activation functions\"\n        }), \" that are good for different things.\\n\", _jsx(_components.sup, {\n          children: _jsx(_components.a, {\n            href: \"#user-content-fn-activation\",\n            id: \"user-content-fnref-activation\",\n            \"data-footnote-ref\": true,\n            \"aria-describedby\": \"footnote-label\",\n            children: \"5\"\n          })\n        })]\n      }), \"\\n\", _jsx(_components.img, {\n        src: \"/images/napkin/problem-17-neural-nets/relu.png\",\n        alt: \"\",\n        width: \"400\",\n        height: \"333\",\n        blurDataURL: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAQAAAADCAIAAAA7ljmRAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAMElEQVR4nGMwMDI0MTVpbGjUMtBlmDhhwv///xcuXxYVH8tgYGRoZGmupqPV29sLABM2DkTEfNKgAAAAAElFTkSuQmCC\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"We can apply this simple operation to our neural net:\"\n      }), \"\\n\", _jsx(_components.pre, {\n        className: \"language-python\",\n        children: _jsxs(_components.code, {\n          className: \"language-python\",\n          children: [_jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"def\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token function\",\n            children: \"model\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n    output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"=\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0.\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"for\"\n          }), \" index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"in\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"enumerate\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), \"rectangle\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \":\"\n          }), \"\\n        output_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"+=\"\n          }), \" input_neuron \", _jsx(_components.span, {\n            className: \"token operator\",\n            children: \"*\"\n          }), \" hidden_layer\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"[\"\n          }), \"index\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"]\"\n          }), \"\\n    \", _jsx(_components.span, {\n            className: \"token keyword\",\n            children: \"return\"\n          }), \" \", _jsx(_components.span, {\n            className: \"token builtin\",\n            children: \"max\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \"(\"\n          }), _jsx(_components.span, {\n            className: \"token number\",\n            children: \"0\"\n          }), _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \",\"\n          }), \" output_neuron\", _jsx(_components.span, {\n            className: \"token punctuation\",\n            children: \")\"\n          }), \"\\n\"]\n        })\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Now, we only have a single neuron/weight… that isn’t much. Good models have\\n100s, and the biggest models like GPT-3 have billions. So this won’t recognize\\nmany digits or cats, but you can easily add more weights!\"\n      }), \"\\n\", _jsxs(_components.h3, {\n        id: \"matrices\",\n        children: [\"Matrices\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#matrices\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsxs(_components.p, {\n        children: [\"The core operation in our model, the for loop, is matrix multiplication. We could\\nrewrite it to use them instead, e.g. \", _jsx(_components.code, {\n          children: \"rectangle @ hidden_layer\"\n        }), \". PyTorch will\\nthen do the exact same thing. Except, it’ll now execute in C-land. And if you\\nhave a GPU and pass some extra weights, it’ll execute on a GPU, which is even\\nfaster. When doing any kind of deep learning, you want to avoid writing any\\nPython loops. They’re just too slow. If you ran the code above for the 300\\nepochs, you’ll see that it takes minutes to complete. I left matrices out of it\\nto simplify the explanation as much as possible. There’s plenty going on without\\nthem.\"]\n      }), \"\\n\", _jsxs(_components.h2, {\n        id: \"next-steps-to-implement-your-own-neural-net-from-scratch\",\n        children: [\"Next steps to implement your own neural net from scratch\", _jsx(_components.a, {\n          className: \"autolink-header\",\n          \"aria-hidden\": true,\n          tabIndex: \"-1\",\n          href: \"#next-steps-to-implement-your-own-neural-net-from-scratch\",\n          children: _jsx(_components.svg, {\n            className: \"autolink-svg\",\n            xmlns: \"http://www.w3.org/2000/svg\",\n            width: \"18\",\n            height: \"18\",\n            fill: \"currentColor\",\n            viewBox: \"0 0 24 24\",\n            children: _jsx(_components.path, {\n              d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n            })\n          })\n        })]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"Even if you’ve carefully read through this article, you won’t fully grasp it\\nyet until you’ve had your own hands on it. Here are some suggestions on where to\\ngo from here, if you’d like to move beyond the basic understanding you have now:\"\n      }), \"\\n\", _jsxs(_components.ol, {\n        children: [\"\\n\", _jsxs(_components.li, {\n          children: [\"Get the \", _jsx(_components.a, {\n            href: \"https://colab.research.google.com/drive/1YRp9k_ORH4wZMqXLNkc3Ir5w4B5f-8Pa?usp=sharing\",\n            children: \"notebook\"\n          }), \" running and study the code\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Change it to far larger rectangles, e.g. 100x100\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Add biases in addition to the weights. A model doesn’t just have\\nweights that are multiplied onto the inputs, but also biases that are added\\n(\", _jsx(_components.code, {\n            children: \"+\"\n          }), \") onto the inputs in each layer.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Rewrite the model to use \", _jsx(_components.a, {\n            href: \"https://pytorch.org/docs/stable/tensors.html\",\n            children: \"PyTorch tensors\"\n          }), \" for matrix operations, as\\ndescribed in the previous section.\"]\n        }), \"\\n\", _jsx(_components.li, {\n          children: \"Add 1-2 more layers to the model. Try to have them have different sizes.\"\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"Change the tensors to run on GPU (see the \", _jsx(_components.a, {\n            href: \"https://pytorch.org/docs/stable/notes/cuda.html\",\n            children: \"PyTorch\\ndocumentation\"\n          }), \") and see the\\nperformance speed up! Increase the size of the training set and rectangles to\\n\", _jsx(_components.em, {\n            children: \"really\"\n          }), \" be able to tell the difference. Make sure you change \", _jsx(_components.code, {\n            children: \"Runtime \u003e Change Runtime Type\"\n          }), \" in Collab to run on a GPU.\"]\n        }), \"\\n\", _jsxs(_components.li, {\n          children: [\"This is a difficult step that will likely take a while, but it’ll be well\\nworth it: Adapt the code to recognize handwritten letters from the \", _jsx(_components.a, {\n            href: \"https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz\",\n            children: \"MNIST\\ndataset\"\n          }), \" dataset. You’ll need to use \", _jsx(_components.a, {\n            href: \"https://pillow.readthedocs.io/en/stable/\",\n            children: _jsx(_components.code, {\n              children: \"pillow\"\n            })\n          }), \" to turn\\nthe pixels into a large 1-dimensional tensor as the input layer, as well as a\\nnon-linear activation function like \", _jsx(_components.code, {\n            children: \"Sigmoid\"\n          }), \" or \", _jsx(_components.code, {\n            children: \"ReLU\"\n          }), \". Use \", _jsx(_components.a, {\n            href: \"http://neuralnetworksanddeeplearning.com/\",\n            children: \"Nielsen’s\\nbook\"\n          }), \" as a reference if you get stuck, which does exactly this.\"]\n        }), \"\\n\"]\n      }), \"\\n\", _jsx(_components.p, {\n        children: \"I thoroughly hope you enjoyed this walkthrough of a neural net from scratch! In\\na future issue we’ll use the mental model we’ve built up here to do some napkin\\nmath on expected performance on training and using neural nets.\"\n      }), \"\\n\", _jsx(_components.p, {\n        children: _jsxs(_components.em, {\n          children: [\"Thanks to \", _jsx(_components.a, {\n            href: \"https://www.vegardstikbakke.com/\",\n            children: \"Vegard Stikbakke\"\n          }), \", \", _jsx(_components.a, {\n            href: \"https://www.flyingcroissant.ca/\",\n            children: \"Andrew Bugera\"\n          }), \" and \", _jsx(_components.a, {\n            href: \"https://thundergolfer.com/\",\n            children: \"Jonathan\\nBelotti\"\n          }), \" for providing valuable feedback on drafts of this article.\"]\n        })\n      }), \"\\n\", _jsxs(_components.section, {\n        \"data-footnotes\": true,\n        className: \"footnotes\",\n        children: [_jsxs(_components.h2, {\n          id: \"footnote-label\",\n          className: \"sr-only\",\n          children: [\"Footnotes\", _jsx(_components.a, {\n            className: \"autolink-header\",\n            \"aria-hidden\": true,\n            tabIndex: \"-1\",\n            href: \"#footnote-label\",\n            children: _jsx(_components.svg, {\n              className: \"autolink-svg\",\n              xmlns: \"http://www.w3.org/2000/svg\",\n              width: \"18\",\n              height: \"18\",\n              fill: \"currentColor\",\n              viewBox: \"0 0 24 24\",\n              children: _jsx(_components.path, {\n                d: \"M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z\"\n              })\n            })\n          })]\n        }), \"\\n\", _jsxs(_components.ol, {\n          children: [\"\\n\", _jsxs(_components.li, {\n            id: \"user-content-fn-google\",\n            children: [\"\\n\", _jsxs(_components.p, {\n              children: [\"This is a good example of \", _jsx(\"a\", {\n                href: \"/peak-complexity\",\n                children: \"Peak Complexity\"\n              }), \".\\nThe existing phrase-based translation model was iteratively improved with\\nincreasing complexity, distributed systems to look up five-word phrases\\nfrequencies, etc. The complexity required to improve the model 1% was becoming\\nastronomical. A good hint you need a paradigm shift to reset the complexity.\\nDeep Learning provided that complexity reset for the translation model. \", _jsx(_components.a, {\n                href: \"#user-content-fnref-google\",\n                \"data-footnote-backref\": true,\n                className: \"data-footnote-backref\",\n                \"aria-label\": \"Back to content\",\n                children: \"↩\"\n              })]\n            }), \"\\n\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            id: \"user-content-fn-gpt3\",\n            children: [\"\\n\", _jsxs(_components.p, {\n              children: [\"GPT-3 has ~175 billion weights. The human brain has ~86 billion. Of\\ncourse, you cannot technically compare an artificial neuron to a real one.\\nWhy? I don’t know. I reserve that it remains an interesting question. \", _jsx(_components.a, {\n                href: \"https://lastweekin.ai/p/gpt-3-is-no-longer-the-only-game\",\n                children: \"It’s\\nestimated\"\n              }), \" that it cost in the double-digit millions to train it. \", _jsx(_components.a, {\n                href: \"#user-content-fnref-gpt3\",\n                \"data-footnote-backref\": true,\n                className: \"data-footnote-backref\",\n                \"aria-label\": \"Back to content\",\n                children: \"↩\"\n              })]\n            }), \"\\n\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            id: \"user-content-fn-3blue1brown\",\n            children: [\"\\n\", _jsxs(_components.p, {\n              children: [\"There’s a brilliant \", _jsx(_components.a, {\n                href: \"https://www.youtube.com/watch?v=aircAruvnKk\",\n                children: \"Youtube series\"\n              }), \" that’ll go\\ninto more depth on the math than I do in this article. This article\\naccompanies the video nicely, as the video doesn’t go into the implementation. \", _jsx(_components.a, {\n                href: \"#user-content-fnref-3blue1brown\",\n                \"data-footnote-backref\": true,\n                className: \"data-footnote-backref\",\n                \"aria-label\": \"Back to content\",\n                children: \"↩\"\n              })]\n            }), \"\\n\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            id: \"user-content-fn-nielsen\",\n            children: [\"\\n\", _jsxs(_components.p, {\n              children: [\"There’s a great, \", _jsx(_components.a, {\n                href: \"http://neuralnetworksanddeeplearning.com/\",\n                children: \"short e-book\"\n              }), \" on implementing a neural\\nnetwork from scratch available that goes into far more detail on computing the\\nderivative from scratch. Despite this existing, I still decided to do this\\nwrite-up because calculating the slope manually takes up a lot of time and\\ncomplexity. I wanted to teach it from scratch without going into those\\ndetails. \", _jsx(_components.a, {\n                href: \"#user-content-fnref-nielsen\",\n                \"data-footnote-backref\": true,\n                className: \"data-footnote-backref\",\n                \"aria-label\": \"Back to content\",\n                children: \"↩\"\n              })]\n            }), \"\\n\"]\n          }), \"\\n\", _jsxs(_components.li, {\n            id: \"user-content-fn-activation\",\n            children: [\"\\n\", _jsxs(_components.p, {\n              children: [\"I found this pretty strange when I learned about neural networks.\\nWe can use a bunch of random non-linear function and our neural network\\nworks… better? The answer is yes! The complicated answer I am not\\nknowledgeable enough to offer… If you write your own handwritten MNIST\\nneural net (as suggested at the end of the article), you can see for yourself\\nby adding/removing a non-linear function and looking at the loss. \", _jsx(_components.a, {\n                href: \"#user-content-fnref-activation\",\n                \"data-footnote-backref\": true,\n                className: \"data-footnote-backref\",\n                \"aria-label\": \"Back to content\",\n                children: \"↩\"\n              })]\n            }), \"\\n\"]\n          }), \"\\n\"]\n        }), \"\\n\"]\n      })]\n    });\n  }\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{}},"noHeader":false,"noFooter":false,"noMenu":false,"noToc":false,"category":"code","date":"2022-01-03","title":"Napkin Problem 18: Neural Network From Scratch"},"recommendations":[{"title":"MySQL transactions per second vs fsyncs per second","id":"napkin/problem-10-mysql-transactions-per-second"},{"title":"Using checksums to verify syncing 100M database records","id":"napkin/problem-14-using-checksums-to-verify"},{"title":"Shitlist Driven Development","id":"shitlists"},{"title":"Increase HTTP Performance by Fitting In the Initial TCP Slow Start Window","id":"napkin/problem-15"},{"title":"Inverted Index Performance and Merkle Tree Syncronization","id":"napkin/problem-9"}],"emailSubscribers":2873,"fallback":{}},"__N_SSG":true},"page":"/[...id]","query":{"id":["napkin","neural-net"]},"buildId":"JWaI_7syLZf-2cqKst7XY","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>